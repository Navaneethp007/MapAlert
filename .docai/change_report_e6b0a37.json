{
  "affected_elements": {
    "added": [
      {
        "type": "class",
        "name": "CodeAnalyzer",
        "start_line": 65,
        "end_line": 674,
        "code": "class CodeAnalyzer:\n    \"\"\"Analyzes code changes in a git repository.\"\"\"\n\n    def __init__(self, repo_path: str):\n        \"\"\"\n        Initialize the CodeAnalyzer.\n\n        Args:\n            repo_path: Path to the git repository\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = Repo(repo_path)\n        self.languages = self._setup_tree_sitter()\n        self.parser = Parser()\n        self.code_elements_db = self._load_code_elements_db()\n        self._custom_commit_range = None\n\n    def _setup_tree_sitter(self) -> Dict[str, Language]:\n        \"\"\"Initialize and load tree-sitter languages.\"\"\"\n        lang_objects = {}\n        for lang_name, lang_module in LANGUAGE_MODULES.items():\n            try:\n                if lang_module():\n                    lang_objects[lang_name] = Language(lang_module())\n            except Exception as e:\n                print(f\"Error loading language {lang_name}: {e}\")\n        return lang_objects\n\n    def _load_code_elements_db(self) -> Dict:\n        \"\"\"Load existing code elements database if it exists.\"\"\"\n        if os.path.exists(ELEMENTS_DB_PATH):\n            try:\n                with open(ELEMENTS_DB_PATH, \"r\", encoding=\"utf-8\") as f:\n                    return json.load(f)\n            except json.JSONDecodeError:\n                print(\n                    f\"Warning: Invalid JSON in {ELEMENTS_DB_PATH}, creating new database\"\n                )\n\n        # Return empty database structure if file doesn't exist or is invalid\n        return {\"elements\": {}, \"metadata\": {\"last_processed_commit\": None}}\n\n    def _save_code_elements_db(self):\n        \"\"\"Save the code elements database.\"\"\"\n        with open(ELEMENTS_DB_PATH, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.code_elements_db, f, indent=2)\n\n    def _get_file_language(self, file_path: str) -> Optional[str]:\n        \"\"\"\n        Determine the programming language of a file based on its extension.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            Language name or None if not supported\n        \"\"\"\n        extension = os.path.splitext(file_path)[1].lower()\n        return LANGUAGE_EXTENSIONS.get(extension)\n\n    def _get_push_commits(self) -> Tuple[str, str]:\n        \"\"\"\n        Get the commit range for the current push.\n\n        For GitHub Actions, determines the before and after commits of the push.\n        Can be overridden by command-line arguments.\n\n        Returns:\n            Tuple of (before_commit, after_commit)\n        \"\"\"\n        # If custom commit range is provided, use it\n        if self._custom_commit_range:\n            return self._custom_commit_range\n\n        # In GitHub Actions environment\n        if os.environ.get(\"GITHUB_EVENT_NAME\") == \"push\":\n            # If this is a push event, get the before and after SHAs\n            try:\n                # Try to get the before and after commits from the GitHub context\n                event_path = os.environ.get(\"GITHUB_EVENT_PATH\")\n                if event_path and os.path.exists(event_path):\n                    with open(event_path, \"r\") as f:\n                        event_data = json.load(f)\n                        before_commit = event_data.get(\"before\")\n                        after_commit = event_data.get(\"after\")\n                        if before_commit and after_commit:\n                            return before_commit, after_commit\n            except Exception as e:\n                print(f\"Error reading GitHub event data: {e}\")\n\n        # Get the first commit in the repository as the \"before\" commit\n        try:\n            # Get the first commit\n            first_commit = None\n            for commit in self.repo.iter_commits(\"--all\", max_parents=0):\n                first_commit = commit.hexsha\n                break  # Just need the first one\n\n            if not first_commit:\n                # Empty repository case - use git's empty tree object\n                first_commit = self.repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n\n            # Use HEAD as the \"after\" commit\n            last_commit = self.repo.head.commit.hexsha\n\n            return first_commit, last_commit\n        except Exception as e:\n            print(f\"Error determining commit range: {e}\")\n            # If all else fails, dynamically create an empty tree object\n            empty_tree = self.repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n            return empty_tree, \"HEAD\"\n\n    def _get_affected_files(\n        self, before_commit: str, after_commit: str\n    ) -> Dict[str, List[str]]:\n        \"\"\"\n        Get files affected (modified, added, deleted) between two commits.\n\n        Args:\n            before_commit: Starting commit hash\n            after_commit: Ending commit hash\n\n        Returns:\n            Dictionary with lists of modified, added, and deleted files\n        \"\"\"\n        result = {\"modified\": [], \"added\": [], \"deleted\": []}\n\n        # Handle first commit case - check if before_commit is an empty tree\n        try:\n            # Try to get the commit - will fail if it's an empty tree\n            self.repo.commit(before_commit)\n        except Exception:\n            # Likely an empty tree - all files are new\n            diff_index = self.repo.git.diff(\n                \"--name-status\", before_commit, after_commit\n            )\n            for line in diff_index.splitlines():\n                parts = line.split(\"\\t\")\n                if len(parts) >= 2:\n                    status, filename = parts[0], parts[1]\n                    if status == \"A\":\n                        file_path = os.path.join(self.repo_path, filename)\n                        result[\"added\"].append(file_path)\n            return result\n\n        # Get diff between commits\n        diff_index = self.repo.git.diff(\"--name-status\", before_commit, after_commit)\n\n        for line in diff_index.splitlines():\n            parts = line.split(\"\\t\")\n            if len(parts) >= 2:\n                status, filename = parts[0], parts[1]\n                file_path = os.path.join(self.repo_path, filename)\n\n                if status == \"M\":\n                    result[\"modified\"].append(file_path)\n                elif status == \"A\":\n                    result[\"added\"].append(file_path)\n                elif status == \"D\":\n                    result[\"deleted\"].append(file_path)\n                elif status.startswith(\"R\"):\n                    # Handle renamed files (old name is deleted, new name is added)\n                    if len(parts) >= 3:\n                        old_filename, new_filename = parts[1], parts[2]\n                        old_path = os.path.join(self.repo_path, old_filename)\n                        new_path = os.path.join(self.repo_path, new_filename)\n                        result[\"deleted\"].append(old_path)\n                        result[\"added\"].append(new_path)\n\n        return result\n\n    def _get_affected_lines(\n        self, file_path: str, before_commit: str, after_commit: str\n    ) -> Set[int]:\n        \"\"\"\n        Get line numbers affected by changes between two commits for a specific file.\n\n        Args:\n            file_path: Path to the file\n            before_commit: Starting commit hash\n            after_commit: Ending commit hash\n\n        Returns:\n            Set of affected line numbers\n        \"\"\"\n        rel_path = os.path.relpath(file_path, self.repo_path)\n        affected_lines = set()\n\n        try:\n            # Get unified diff with context\n            diff_output = self.repo.git.diff(\n                before_commit, after_commit, rel_path, unified=0\n            )\n\n            for line in diff_output.splitlines():\n                if line.startswith(\"@@\"):\n                    # Parse the @@ -a,b +c,d @@ line\n                    parts = line.split()\n                    if len(parts) >= 2:\n                        line_info = parts[1]  # +c,d part\n                        if line_info.startswith(\"+\"):\n                            line_info = line_info[1:]  # Remove the + sign\n                            if \",\" in line_info:\n                                start_line, num_lines = map(int, line_info.split(\",\"))\n                            else:\n                                start_line, num_lines = int(line_info), 1\n\n                            # Add all affected lines to the set\n                            affected_lines.update(\n                                range(start_line, start_line + num_lines)\n                            )\n        except GitCommandError:\n            # File might be new or deleted, consider all lines affected\n            return set(range(1, 100000))\n        except Exception as e:\n            print(f\"Error getting affected lines for {file_path}: {e}\")\n            return set(range(1, 100000))\n\n        return affected_lines\n\n    def _get_file_content_at_commit(\n        self, file_path: str, commit: str\n    ) -> Optional[bytes]:\n        \"\"\"\n        Get the content of a file at a specific commit.\n\n        Args:\n            file_path: Path to the file\n            commit: Commit hash\n\n        Returns:\n            File content as bytes or None if not found\n        \"\"\"\n        rel_path = os.path.relpath(file_path, self.repo_path)\n\n        try:\n            # Get the file content at the specific commit\n            content = self.repo.git.show(f\"{commit}:{rel_path}\")\n            return content.encode(\"utf-8\", errors=\"replace\")\n        except GitCommandError:\n            # File might not exist at this commit\n            return None\n        except Exception as e:\n            print(f\"Error getting file content for {file_path} at {commit}: {e}\")\n            return None\n\n    def _parse_file(\n        self, file_path: str, content: Optional[bytes] = None\n    ) -> List[Dict]:\n        \"\"\"\n        Parse a file to extract code elements (functions, classes, methods).\n\n        Args:\n            file_path: Path to the file\n            content: Optional file content (bytes), reads from file if None\n\n        Returns:\n            List of code elements\n        \"\"\"\n        lang_name = self._get_file_language(file_path)\n        if not lang_name or lang_name not in self.languages:\n            return []\n\n        # Get file content if not provided\n        if content is None:\n            try:\n                with open(file_path, \"rb\") as f:\n                    content = f.read()\n            except Exception as e:\n                print(f\"Error reading file {file_path}: {e}\")\n                return []\n\n        # Define queries for different languages\n        queries = {\n            \"python\": \"\"\"\n                (function_definition\n                  name: (identifier) @function_name) @function\n\n                (class_definition\n                  name: (identifier) @class_name) @class\n            \"\"\",\n            \"javascript\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (class_declaration\n                  name: (identifier) @class_name) @class\n\n                (method_definition\n                  name: (property_identifier) @method_name) @method\n\n                (arrow_function\n                  parameters: (formal_parameters) @params) @arrow_function\n            \"\"\",\n            \"typescript\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (class_declaration\n                  name: (type_identifier) @class_name) @class\n\n                (method_definition\n                  name: (property_identifier) @method_name) @method\n            \"\"\",\n            \"java\": \"\"\"\n                (method_declaration\n                  name: (identifier) @method_name) @method\n\n                (class_declaration\n                  name: (identifier) @class_name) @class\n            \"\"\",\n            \"go\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (method_declaration\n                  name: (field_identifier) @method_name) @method\n            \"\"\",\n            \"rust\": \"\"\"\n                (function_item\n                  name: (identifier) @function_name) @function\n\n                (impl_item\n                  name: (identifier) @impl_name) @impl\n            \"\"\",\n        }\n\n        if lang_name not in queries:\n            return []\n\n        # Parse the file\n        self.parser.language = self.languages[lang_name]\n        tree = self.parser.parse(content)\n\n        print(f\"Parsing file: {file_path}\")\n\n        # Try direct approach using the node iterator\n        code_elements = []\n\n        # Find function definitions\n        for node in self._iter_tree(tree.root_node):\n            element_type = None\n            element_name = None\n\n            # Check node type to determine element type\n            if node.type == \"function_definition\":\n                element_type = \"function\"\n            elif node.type == \"class_definition\":\n                element_type = \"class\"\n            elif node.type == \"method_definition\":\n                element_type = \"method\"\n            elif node.type == \"function_declaration\":\n                element_type = \"function\"\n            elif node.type == \"class_declaration\":\n                element_type = \"class\"\n\n            if element_type:\n                # Find the name node\n                for child in node.children:\n                    if (\n                        child.type == \"identifier\"\n                        or child.type == \"property_identifier\"\n                    ):\n                        element_name = content[\n                            child.start_byte : child.end_byte\n                        ].decode(\"utf-8\", errors=\"replace\")\n                        break\n\n                if element_name:\n                    # Get the full code\n                    element_code = content[node.start_byte : node.end_byte].decode(\n                        \"utf-8\", errors=\"replace\"\n                    )\n\n                    # Use relative path for file_path\n                    rel_file_path = os.path.relpath(file_path, self.repo_path)\n\n                    # Create the element\n                    code_elements.append(\n                        {\n                            \"type\": element_type,\n                            \"name\": element_name,\n                            \"start_line\": node.start_point[0]\n                            + 1,  # 1-based line numbering\n                            \"end_line\": node.end_point[0] + 1,\n                            \"code\": element_code,\n                            \"file_path\": rel_file_path,\n                        }\n                    )\n                    print(f\"Found {element_type}: {element_name}\")\n\n        return code_elements\n\n    def _iter_tree(self, node):\n        \"\"\"Helper method to iterate through all nodes in a tree.\"\"\"\n        yield node\n        for child in node.children:\n            yield from self._iter_tree(child)\n\n    def _parse_file_at_commit(self, file_path: str, commit: str) -> List[Dict]:\n        \"\"\"\n        Parse a file at a specific commit to extract code elements.\n\n        Args:\n            file_path: Path to the file\n            commit: Commit hash\n\n        Returns:\n            List of code elements\n        \"\"\"\n        content = self._get_file_content_at_commit(file_path, commit)\n        if not content:\n            return []\n\n        return self._parse_file(file_path, content)\n\n    def _generate_element_id(self, element: Dict) -> str:\n        \"\"\"\n        Generate a unique ID for a code element.\n\n        Args:\n            element: Code element dictionary\n\n        Returns:\n            Unique ID string\n        \"\"\"\n        rel_path = os.path.relpath(element['file_path'], self.repo_path)\n        unique_str = f\"{rel_path}:{element['name']}:{element['type']}\"\n        return hashlib.md5(unique_str.encode()).hexdigest()\n\n    def analyze_repo_changes(self) -> Dict:\n        \"\"\"\n        Analyze changes in the repository.\n\n        Returns:\n            Dictionary with added, modified, and deleted code elements\n        \"\"\"\n        before_commit, after_commit = self._get_push_commits()\n        print(f\"Analyzing changes between {before_commit} and {after_commit}\")\n\n        # Get affected files\n        affected_files = self._get_affected_files(before_commit, after_commit)\n\n        # Lists to track code elements\n        added_elements = []\n        modified_elements = []\n        deleted_elements = []\n\n        # Process modified files\n        for file_path in affected_files[\"modified\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get affected lines\n            affected_lines = self._get_affected_lines(\n                file_path, before_commit, after_commit\n            )\n\n            # Get elements from before commit\n            before_elements = self._parse_file_at_commit(file_path, before_commit)\n            before_elements_map = {\n                self._generate_element_id(e): e for e in before_elements\n            }\n\n            # Get elements from after commit\n            after_elements = self._parse_file(file_path)\n            after_elements_map = {\n                self._generate_element_id(e): e for e in after_elements\n            }\n\n            # Find added and modified elements\n            for element_id, after_element in after_elements_map.items():\n                # Always consider the element affected for now\n                # This is a simplification to ensure elements are captured\n                is_affected = True\n\n                if element_id in before_elements_map:\n                    # Element exists in both commits - check if modified\n                    before_element = before_elements_map[element_id]\n\n                    # Only add if the code has actually changed\n                    if before_element[\"code\"] != after_element[\"code\"]:\n                        modified_elements.append(after_element)\n\n                        # Update the elements database\n                        self.code_elements_db[\"elements\"][element_id] = {\n                            \"type\": after_element[\"type\"],\n                            \"name\": after_element[\"name\"],\n                            \"file_path\": after_element[\"file_path\"],\n                            \"code\": after_element[\"code\"],\n                            \"last_modified\": after_commit,\n                        }\n                        print(\n                            f\"Modified element: {after_element['type']} {after_element['name']}\"\n                        )\n                else:\n                    # Element exists only in after commit - added\n                    added_elements.append(after_element)\n\n                    # Add to the elements database\n                    self.code_elements_db[\"elements\"][element_id] = {\n                        \"type\": after_element[\"type\"],\n                        \"name\": after_element[\"name\"],\n                        \"file_path\": after_element[\"file_path\"],\n                        \"code\": after_element[\"code\"],\n                        \"added_at\": after_commit,\n                    }\n                    print(\n                        f\"Added element: {after_element['type']} {after_element['name']}\"\n                    )\n\n            # Find deleted elements\n            for element_id, before_element in before_elements_map.items():\n                if element_id not in after_elements_map:\n                    deleted_elements.append(before_element)\n\n                    # Mark as deleted in the database if it exists\n                    if element_id in self.code_elements_db[\"elements\"]:\n                        self.code_elements_db[\"elements\"][element_id][\n                            \"deleted_at\"\n                        ] = after_commit\n                    print(\n                        f\"Deleted element: {before_element['type']} {before_element['name']}\"\n                    )\n\n        # Process added files\n        for file_path in affected_files[\"added\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get elements from the new file\n            elements = self._parse_file(file_path)\n\n            for element in elements:\n                element_id = self._generate_element_id(element)\n                added_elements.append(element)\n\n                # Add to the elements database\n                self.code_elements_db[\"elements\"][element_id] = {\n                    \"type\": element[\"type\"],\n                    \"name\": element[\"name\"],\n                    \"file_path\": element[\"file_path\"],\n                    \"code\": element[\"code\"],\n                    \"added_at\": after_commit,\n                }\n                print(\n                    f\"Added element from new file: {element['type']} {element['name']}\"\n                )\n\n        # Process deleted files\n        for file_path in affected_files[\"deleted\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get elements from the deleted file in the previous commit\n            elements = self._parse_file_at_commit(file_path, before_commit)\n\n            for element in elements:\n                element_id = self._generate_element_id(element)\n                deleted_elements.append(element)\n\n                # Mark as deleted in the database if it exists\n                if element_id in self.code_elements_db[\"elements\"]:\n                    self.code_elements_db[\"elements\"][element_id][\n                        \"deleted_at\"\n                    ] = after_commit\n                print(\n                    f\"Deleted element from removed file: {element['type']} {element['name']}\"\n                )\n\n        # Update metadata\n        self.code_elements_db[\"metadata\"][\"last_processed_commit\"] = after_commit\n\n        # Save the updated database\n        self._save_code_elements_db()\n\n        # Generate report JSON\n        report = {\n            \"affected_elements\": {\n                \"added\": added_elements,\n                \"modified\": modified_elements,\n                \"deleted\": deleted_elements,\n            },\n            \"stats\": {\n                \"added\": len(added_elements),\n                \"modified\": len(modified_elements),\n                \"deleted\": len(deleted_elements),\n                \"total_elements\": len(self.code_elements_db[\"elements\"]),\n            },\n            \"metadata\": {\n                \"repository\": os.path.basename(self.repo_path),\n                \"commit_range\": {\"before\": before_commit, \"after\": after_commit},\n                \"timestamp\": self.repo.head.commit.committed_datetime.isoformat(),\n            },\n        }\n\n        # Save the report\n        report_path = os.path.join(DOCAI_DIR, f\"change_report_{after_commit[:7]}.json\")\n        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(report, f, indent=2)\n\n        print(f\"Report saved to {report_path}\")\n        print(\n            f\"Added: {len(added_elements)}, Modified: {len(modified_elements)}, Deleted: {len(deleted_elements)}\"\n        )\n\n        return report",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "start_line": 68,
        "end_line": 80,
        "code": "def __init__(self, repo_path: str):\n        \"\"\"\n        Initialize the CodeAnalyzer.\n\n        Args:\n            repo_path: Path to the git repository\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = Repo(repo_path)\n        self.languages = self._setup_tree_sitter()\n        self.parser = Parser()\n        self.code_elements_db = self._load_code_elements_db()\n        self._custom_commit_range = None",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_setup_tree_sitter",
        "start_line": 82,
        "end_line": 91,
        "code": "def _setup_tree_sitter(self) -> Dict[str, Language]:\n        \"\"\"Initialize and load tree-sitter languages.\"\"\"\n        lang_objects = {}\n        for lang_name, lang_module in LANGUAGE_MODULES.items():\n            try:\n                if lang_module():\n                    lang_objects[lang_name] = Language(lang_module())\n            except Exception as e:\n                print(f\"Error loading language {lang_name}: {e}\")\n        return lang_objects",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_load_code_elements_db",
        "start_line": 93,
        "end_line": 105,
        "code": "def _load_code_elements_db(self) -> Dict:\n        \"\"\"Load existing code elements database if it exists.\"\"\"\n        if os.path.exists(ELEMENTS_DB_PATH):\n            try:\n                with open(ELEMENTS_DB_PATH, \"r\", encoding=\"utf-8\") as f:\n                    return json.load(f)\n            except json.JSONDecodeError:\n                print(\n                    f\"Warning: Invalid JSON in {ELEMENTS_DB_PATH}, creating new database\"\n                )\n\n        # Return empty database structure if file doesn't exist or is invalid\n        return {\"elements\": {}, \"metadata\": {\"last_processed_commit\": None}}",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_save_code_elements_db",
        "start_line": 107,
        "end_line": 110,
        "code": "def _save_code_elements_db(self):\n        \"\"\"Save the code elements database.\"\"\"\n        with open(ELEMENTS_DB_PATH, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.code_elements_db, f, indent=2)",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_get_file_language",
        "start_line": 112,
        "end_line": 123,
        "code": "def _get_file_language(self, file_path: str) -> Optional[str]:\n        \"\"\"\n        Determine the programming language of a file based on its extension.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            Language name or None if not supported\n        \"\"\"\n        extension = os.path.splitext(file_path)[1].lower()\n        return LANGUAGE_EXTENSIONS.get(extension)",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_get_push_commits",
        "start_line": 125,
        "end_line": 175,
        "code": "def _get_push_commits(self) -> Tuple[str, str]:\n        \"\"\"\n        Get the commit range for the current push.\n\n        For GitHub Actions, determines the before and after commits of the push.\n        Can be overridden by command-line arguments.\n\n        Returns:\n            Tuple of (before_commit, after_commit)\n        \"\"\"\n        # If custom commit range is provided, use it\n        if self._custom_commit_range:\n            return self._custom_commit_range\n\n        # In GitHub Actions environment\n        if os.environ.get(\"GITHUB_EVENT_NAME\") == \"push\":\n            # If this is a push event, get the before and after SHAs\n            try:\n                # Try to get the before and after commits from the GitHub context\n                event_path = os.environ.get(\"GITHUB_EVENT_PATH\")\n                if event_path and os.path.exists(event_path):\n                    with open(event_path, \"r\") as f:\n                        event_data = json.load(f)\n                        before_commit = event_data.get(\"before\")\n                        after_commit = event_data.get(\"after\")\n                        if before_commit and after_commit:\n                            return before_commit, after_commit\n            except Exception as e:\n                print(f\"Error reading GitHub event data: {e}\")\n\n        # Get the first commit in the repository as the \"before\" commit\n        try:\n            # Get the first commit\n            first_commit = None\n            for commit in self.repo.iter_commits(\"--all\", max_parents=0):\n                first_commit = commit.hexsha\n                break  # Just need the first one\n\n            if not first_commit:\n                # Empty repository case - use git's empty tree object\n                first_commit = self.repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n\n            # Use HEAD as the \"after\" commit\n            last_commit = self.repo.head.commit.hexsha\n\n            return first_commit, last_commit\n        except Exception as e:\n            print(f\"Error determining commit range: {e}\")\n            # If all else fails, dynamically create an empty tree object\n            empty_tree = self.repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n            return empty_tree, \"HEAD\"",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_get_affected_files",
        "start_line": 177,
        "end_line": 234,
        "code": "def _get_affected_files(\n        self, before_commit: str, after_commit: str\n    ) -> Dict[str, List[str]]:\n        \"\"\"\n        Get files affected (modified, added, deleted) between two commits.\n\n        Args:\n            before_commit: Starting commit hash\n            after_commit: Ending commit hash\n\n        Returns:\n            Dictionary with lists of modified, added, and deleted files\n        \"\"\"\n        result = {\"modified\": [], \"added\": [], \"deleted\": []}\n\n        # Handle first commit case - check if before_commit is an empty tree\n        try:\n            # Try to get the commit - will fail if it's an empty tree\n            self.repo.commit(before_commit)\n        except Exception:\n            # Likely an empty tree - all files are new\n            diff_index = self.repo.git.diff(\n                \"--name-status\", before_commit, after_commit\n            )\n            for line in diff_index.splitlines():\n                parts = line.split(\"\\t\")\n                if len(parts) >= 2:\n                    status, filename = parts[0], parts[1]\n                    if status == \"A\":\n                        file_path = os.path.join(self.repo_path, filename)\n                        result[\"added\"].append(file_path)\n            return result\n\n        # Get diff between commits\n        diff_index = self.repo.git.diff(\"--name-status\", before_commit, after_commit)\n\n        for line in diff_index.splitlines():\n            parts = line.split(\"\\t\")\n            if len(parts) >= 2:\n                status, filename = parts[0], parts[1]\n                file_path = os.path.join(self.repo_path, filename)\n\n                if status == \"M\":\n                    result[\"modified\"].append(file_path)\n                elif status == \"A\":\n                    result[\"added\"].append(file_path)\n                elif status == \"D\":\n                    result[\"deleted\"].append(file_path)\n                elif status.startswith(\"R\"):\n                    # Handle renamed files (old name is deleted, new name is added)\n                    if len(parts) >= 3:\n                        old_filename, new_filename = parts[1], parts[2]\n                        old_path = os.path.join(self.repo_path, old_filename)\n                        new_path = os.path.join(self.repo_path, new_filename)\n                        result[\"deleted\"].append(old_path)\n                        result[\"added\"].append(new_path)\n\n        return result",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_get_affected_lines",
        "start_line": 236,
        "end_line": 283,
        "code": "def _get_affected_lines(\n        self, file_path: str, before_commit: str, after_commit: str\n    ) -> Set[int]:\n        \"\"\"\n        Get line numbers affected by changes between two commits for a specific file.\n\n        Args:\n            file_path: Path to the file\n            before_commit: Starting commit hash\n            after_commit: Ending commit hash\n\n        Returns:\n            Set of affected line numbers\n        \"\"\"\n        rel_path = os.path.relpath(file_path, self.repo_path)\n        affected_lines = set()\n\n        try:\n            # Get unified diff with context\n            diff_output = self.repo.git.diff(\n                before_commit, after_commit, rel_path, unified=0\n            )\n\n            for line in diff_output.splitlines():\n                if line.startswith(\"@@\"):\n                    # Parse the @@ -a,b +c,d @@ line\n                    parts = line.split()\n                    if len(parts) >= 2:\n                        line_info = parts[1]  # +c,d part\n                        if line_info.startswith(\"+\"):\n                            line_info = line_info[1:]  # Remove the + sign\n                            if \",\" in line_info:\n                                start_line, num_lines = map(int, line_info.split(\",\"))\n                            else:\n                                start_line, num_lines = int(line_info), 1\n\n                            # Add all affected lines to the set\n                            affected_lines.update(\n                                range(start_line, start_line + num_lines)\n                            )\n        except GitCommandError:\n            # File might be new or deleted, consider all lines affected\n            return set(range(1, 100000))\n        except Exception as e:\n            print(f\"Error getting affected lines for {file_path}: {e}\")\n            return set(range(1, 100000))\n\n        return affected_lines",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_get_file_content_at_commit",
        "start_line": 285,
        "end_line": 309,
        "code": "def _get_file_content_at_commit(\n        self, file_path: str, commit: str\n    ) -> Optional[bytes]:\n        \"\"\"\n        Get the content of a file at a specific commit.\n\n        Args:\n            file_path: Path to the file\n            commit: Commit hash\n\n        Returns:\n            File content as bytes or None if not found\n        \"\"\"\n        rel_path = os.path.relpath(file_path, self.repo_path)\n\n        try:\n            # Get the file content at the specific commit\n            content = self.repo.git.show(f\"{commit}:{rel_path}\")\n            return content.encode(\"utf-8\", errors=\"replace\")\n        except GitCommandError:\n            # File might not exist at this commit\n            return None\n        except Exception as e:\n            print(f\"Error getting file content for {file_path} at {commit}: {e}\")\n            return None",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_parse_file",
        "start_line": 311,
        "end_line": 456,
        "code": "def _parse_file(\n        self, file_path: str, content: Optional[bytes] = None\n    ) -> List[Dict]:\n        \"\"\"\n        Parse a file to extract code elements (functions, classes, methods).\n\n        Args:\n            file_path: Path to the file\n            content: Optional file content (bytes), reads from file if None\n\n        Returns:\n            List of code elements\n        \"\"\"\n        lang_name = self._get_file_language(file_path)\n        if not lang_name or lang_name not in self.languages:\n            return []\n\n        # Get file content if not provided\n        if content is None:\n            try:\n                with open(file_path, \"rb\") as f:\n                    content = f.read()\n            except Exception as e:\n                print(f\"Error reading file {file_path}: {e}\")\n                return []\n\n        # Define queries for different languages\n        queries = {\n            \"python\": \"\"\"\n                (function_definition\n                  name: (identifier) @function_name) @function\n\n                (class_definition\n                  name: (identifier) @class_name) @class\n            \"\"\",\n            \"javascript\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (class_declaration\n                  name: (identifier) @class_name) @class\n\n                (method_definition\n                  name: (property_identifier) @method_name) @method\n\n                (arrow_function\n                  parameters: (formal_parameters) @params) @arrow_function\n            \"\"\",\n            \"typescript\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (class_declaration\n                  name: (type_identifier) @class_name) @class\n\n                (method_definition\n                  name: (property_identifier) @method_name) @method\n            \"\"\",\n            \"java\": \"\"\"\n                (method_declaration\n                  name: (identifier) @method_name) @method\n\n                (class_declaration\n                  name: (identifier) @class_name) @class\n            \"\"\",\n            \"go\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (method_declaration\n                  name: (field_identifier) @method_name) @method\n            \"\"\",\n            \"rust\": \"\"\"\n                (function_item\n                  name: (identifier) @function_name) @function\n\n                (impl_item\n                  name: (identifier) @impl_name) @impl\n            \"\"\",\n        }\n\n        if lang_name not in queries:\n            return []\n\n        # Parse the file\n        self.parser.language = self.languages[lang_name]\n        tree = self.parser.parse(content)\n\n        print(f\"Parsing file: {file_path}\")\n\n        # Try direct approach using the node iterator\n        code_elements = []\n\n        # Find function definitions\n        for node in self._iter_tree(tree.root_node):\n            element_type = None\n            element_name = None\n\n            # Check node type to determine element type\n            if node.type == \"function_definition\":\n                element_type = \"function\"\n            elif node.type == \"class_definition\":\n                element_type = \"class\"\n            elif node.type == \"method_definition\":\n                element_type = \"method\"\n            elif node.type == \"function_declaration\":\n                element_type = \"function\"\n            elif node.type == \"class_declaration\":\n                element_type = \"class\"\n\n            if element_type:\n                # Find the name node\n                for child in node.children:\n                    if (\n                        child.type == \"identifier\"\n                        or child.type == \"property_identifier\"\n                    ):\n                        element_name = content[\n                            child.start_byte : child.end_byte\n                        ].decode(\"utf-8\", errors=\"replace\")\n                        break\n\n                if element_name:\n                    # Get the full code\n                    element_code = content[node.start_byte : node.end_byte].decode(\n                        \"utf-8\", errors=\"replace\"\n                    )\n\n                    # Use relative path for file_path\n                    rel_file_path = os.path.relpath(file_path, self.repo_path)\n\n                    # Create the element\n                    code_elements.append(\n                        {\n                            \"type\": element_type,\n                            \"name\": element_name,\n                            \"start_line\": node.start_point[0]\n                            + 1,  # 1-based line numbering\n                            \"end_line\": node.end_point[0] + 1,\n                            \"code\": element_code,\n                            \"file_path\": rel_file_path,\n                        }\n                    )\n                    print(f\"Found {element_type}: {element_name}\")\n\n        return code_elements",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_iter_tree",
        "start_line": 458,
        "end_line": 462,
        "code": "def _iter_tree(self, node):\n        \"\"\"Helper method to iterate through all nodes in a tree.\"\"\"\n        yield node\n        for child in node.children:\n            yield from self._iter_tree(child)",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_parse_file_at_commit",
        "start_line": 464,
        "end_line": 479,
        "code": "def _parse_file_at_commit(self, file_path: str, commit: str) -> List[Dict]:\n        \"\"\"\n        Parse a file at a specific commit to extract code elements.\n\n        Args:\n            file_path: Path to the file\n            commit: Commit hash\n\n        Returns:\n            List of code elements\n        \"\"\"\n        content = self._get_file_content_at_commit(file_path, commit)\n        if not content:\n            return []\n\n        return self._parse_file(file_path, content)",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_generate_element_id",
        "start_line": 481,
        "end_line": 493,
        "code": "def _generate_element_id(self, element: Dict) -> str:\n        \"\"\"\n        Generate a unique ID for a code element.\n\n        Args:\n            element: Code element dictionary\n\n        Returns:\n            Unique ID string\n        \"\"\"\n        rel_path = os.path.relpath(element['file_path'], self.repo_path)\n        unique_str = f\"{rel_path}:{element['name']}:{element['type']}\"\n        return hashlib.md5(unique_str.encode()).hexdigest()",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "analyze_repo_changes",
        "start_line": 495,
        "end_line": 674,
        "code": "def analyze_repo_changes(self) -> Dict:\n        \"\"\"\n        Analyze changes in the repository.\n\n        Returns:\n            Dictionary with added, modified, and deleted code elements\n        \"\"\"\n        before_commit, after_commit = self._get_push_commits()\n        print(f\"Analyzing changes between {before_commit} and {after_commit}\")\n\n        # Get affected files\n        affected_files = self._get_affected_files(before_commit, after_commit)\n\n        # Lists to track code elements\n        added_elements = []\n        modified_elements = []\n        deleted_elements = []\n\n        # Process modified files\n        for file_path in affected_files[\"modified\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get affected lines\n            affected_lines = self._get_affected_lines(\n                file_path, before_commit, after_commit\n            )\n\n            # Get elements from before commit\n            before_elements = self._parse_file_at_commit(file_path, before_commit)\n            before_elements_map = {\n                self._generate_element_id(e): e for e in before_elements\n            }\n\n            # Get elements from after commit\n            after_elements = self._parse_file(file_path)\n            after_elements_map = {\n                self._generate_element_id(e): e for e in after_elements\n            }\n\n            # Find added and modified elements\n            for element_id, after_element in after_elements_map.items():\n                # Always consider the element affected for now\n                # This is a simplification to ensure elements are captured\n                is_affected = True\n\n                if element_id in before_elements_map:\n                    # Element exists in both commits - check if modified\n                    before_element = before_elements_map[element_id]\n\n                    # Only add if the code has actually changed\n                    if before_element[\"code\"] != after_element[\"code\"]:\n                        modified_elements.append(after_element)\n\n                        # Update the elements database\n                        self.code_elements_db[\"elements\"][element_id] = {\n                            \"type\": after_element[\"type\"],\n                            \"name\": after_element[\"name\"],\n                            \"file_path\": after_element[\"file_path\"],\n                            \"code\": after_element[\"code\"],\n                            \"last_modified\": after_commit,\n                        }\n                        print(\n                            f\"Modified element: {after_element['type']} {after_element['name']}\"\n                        )\n                else:\n                    # Element exists only in after commit - added\n                    added_elements.append(after_element)\n\n                    # Add to the elements database\n                    self.code_elements_db[\"elements\"][element_id] = {\n                        \"type\": after_element[\"type\"],\n                        \"name\": after_element[\"name\"],\n                        \"file_path\": after_element[\"file_path\"],\n                        \"code\": after_element[\"code\"],\n                        \"added_at\": after_commit,\n                    }\n                    print(\n                        f\"Added element: {after_element['type']} {after_element['name']}\"\n                    )\n\n            # Find deleted elements\n            for element_id, before_element in before_elements_map.items():\n                if element_id not in after_elements_map:\n                    deleted_elements.append(before_element)\n\n                    # Mark as deleted in the database if it exists\n                    if element_id in self.code_elements_db[\"elements\"]:\n                        self.code_elements_db[\"elements\"][element_id][\n                            \"deleted_at\"\n                        ] = after_commit\n                    print(\n                        f\"Deleted element: {before_element['type']} {before_element['name']}\"\n                    )\n\n        # Process added files\n        for file_path in affected_files[\"added\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get elements from the new file\n            elements = self._parse_file(file_path)\n\n            for element in elements:\n                element_id = self._generate_element_id(element)\n                added_elements.append(element)\n\n                # Add to the elements database\n                self.code_elements_db[\"elements\"][element_id] = {\n                    \"type\": element[\"type\"],\n                    \"name\": element[\"name\"],\n                    \"file_path\": element[\"file_path\"],\n                    \"code\": element[\"code\"],\n                    \"added_at\": after_commit,\n                }\n                print(\n                    f\"Added element from new file: {element['type']} {element['name']}\"\n                )\n\n        # Process deleted files\n        for file_path in affected_files[\"deleted\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get elements from the deleted file in the previous commit\n            elements = self._parse_file_at_commit(file_path, before_commit)\n\n            for element in elements:\n                element_id = self._generate_element_id(element)\n                deleted_elements.append(element)\n\n                # Mark as deleted in the database if it exists\n                if element_id in self.code_elements_db[\"elements\"]:\n                    self.code_elements_db[\"elements\"][element_id][\n                        \"deleted_at\"\n                    ] = after_commit\n                print(\n                    f\"Deleted element from removed file: {element['type']} {element['name']}\"\n                )\n\n        # Update metadata\n        self.code_elements_db[\"metadata\"][\"last_processed_commit\"] = after_commit\n\n        # Save the updated database\n        self._save_code_elements_db()\n\n        # Generate report JSON\n        report = {\n            \"affected_elements\": {\n                \"added\": added_elements,\n                \"modified\": modified_elements,\n                \"deleted\": deleted_elements,\n            },\n            \"stats\": {\n                \"added\": len(added_elements),\n                \"modified\": len(modified_elements),\n                \"deleted\": len(deleted_elements),\n                \"total_elements\": len(self.code_elements_db[\"elements\"]),\n            },\n            \"metadata\": {\n                \"repository\": os.path.basename(self.repo_path),\n                \"commit_range\": {\"before\": before_commit, \"after\": after_commit},\n                \"timestamp\": self.repo.head.commit.committed_datetime.isoformat(),\n            },\n        }\n\n        # Save the report\n        report_path = os.path.join(DOCAI_DIR, f\"change_report_{after_commit[:7]}.json\")\n        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(report, f, indent=2)\n\n        print(f\"Report saved to {report_path}\")\n        print(\n            f\"Added: {len(added_elements)}, Modified: {len(modified_elements)}, Deleted: {len(deleted_elements)}\"\n        )\n\n        return report",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "main",
        "start_line": 677,
        "end_line": 734,
        "code": "def main():\n    try:\n        # Set up argument parser\n        parser = argparse.ArgumentParser(\n            description=\"Analyze code changes between commits\"\n        )\n        parser.add_argument(\n            \"--before\", help=\"Starting commit hash (defaults to first commit in repo)\"\n        )\n        parser.add_argument(\"--after\", help=\"Ending commit hash (defaults to HEAD)\")\n        args = parser.parse_args()\n\n        print(\"Starting code analysis...\")\n        analyzer = CodeAnalyzer(REPO_PATH)\n\n        # If commit hashes are provided via command line, use them\n        if args.before is not None or args.after is not None:\n            repo = Repo(REPO_PATH)\n\n            # Handle before commit\n            if args.before is None:\n                # If before is not provided, use the first commit in the repository\n                try:\n                    # Find commits with no parents (root commits)\n                    first_commit = None\n                    for commit in repo.iter_commits(\"--all\", max_parents=0):\n                        first_commit = commit.hexsha\n                        break  # Just need the first one\n\n                    if first_commit:\n                        before_commit = first_commit\n                        print(f\"Using first commit as before: {before_commit}\")\n                    else:\n                        # Empty repository case - use git's empty tree object\n                        before_commit = repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n                except Exception as e:\n                    print(f\"Error finding first commit: {e}\")\n                    before_commit = repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n            else:\n                before_commit = args.before\n\n            # Handle after commit\n            if args.after is None:\n                after_commit = \"HEAD\"\n            else:\n                after_commit = args.after\n\n            # Override the commit range\n            analyzer._custom_commit_range = (before_commit, after_commit)\n\n        analyzer.analyze_repo_changes()\n        print(\"Code analysis completed successfully\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n        import traceback\n\n        traceback.print_exc()\n        sys.exit(1)",
        "file_path": ".github/scripts/analyze_code_changes.py"
      },
      {
        "type": "class",
        "name": "CodeAnalyzer",
        "start_line": 65,
        "end_line": 674,
        "code": "class CodeAnalyzer:\n    \"\"\"Analyzes code changes in a git repository.\"\"\"\n\n    def __init__(self, repo_path: str):\n        \"\"\"\n        Initialize the CodeAnalyzer.\n\n        Args:\n            repo_path: Path to the git repository\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = Repo(repo_path)\n        self.languages = self._setup_tree_sitter()\n        self.parser = Parser()\n        self.code_elements_db = self._load_code_elements_db()\n        self._custom_commit_range = None\n\n    def _setup_tree_sitter(self) -> Dict[str, Language]:\n        \"\"\"Initialize and load tree-sitter languages.\"\"\"\n        lang_objects = {}\n        for lang_name, lang_module in LANGUAGE_MODULES.items():\n            try:\n                if lang_module():\n                    lang_objects[lang_name] = Language(lang_module())\n            except Exception as e:\n                print(f\"Error loading language {lang_name}: {e}\")\n        return lang_objects\n\n    def _load_code_elements_db(self) -> Dict:\n        \"\"\"Load existing code elements database if it exists.\"\"\"\n        if os.path.exists(ELEMENTS_DB_PATH):\n            try:\n                with open(ELEMENTS_DB_PATH, \"r\", encoding=\"utf-8\") as f:\n                    return json.load(f)\n            except json.JSONDecodeError:\n                print(\n                    f\"Warning: Invalid JSON in {ELEMENTS_DB_PATH}, creating new database\"\n                )\n\n        # Return empty database structure if file doesn't exist or is invalid\n        return {\"elements\": {}, \"metadata\": {\"last_processed_commit\": None}}\n\n    def _save_code_elements_db(self):\n        \"\"\"Save the code elements database.\"\"\"\n        with open(ELEMENTS_DB_PATH, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.code_elements_db, f, indent=2)\n\n    def _get_file_language(self, file_path: str) -> Optional[str]:\n        \"\"\"\n        Determine the programming language of a file based on its extension.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            Language name or None if not supported\n        \"\"\"\n        extension = os.path.splitext(file_path)[1].lower()\n        return LANGUAGE_EXTENSIONS.get(extension)\n\n    def _get_push_commits(self) -> Tuple[str, str]:\n        \"\"\"\n        Get the commit range for the current push.\n\n        For GitHub Actions, determines the before and after commits of the push.\n        Can be overridden by command-line arguments.\n\n        Returns:\n            Tuple of (before_commit, after_commit)\n        \"\"\"\n        # If custom commit range is provided, use it\n        if self._custom_commit_range:\n            return self._custom_commit_range\n\n        # In GitHub Actions environment\n        if os.environ.get(\"GITHUB_EVENT_NAME\") == \"push\":\n            # If this is a push event, get the before and after SHAs\n            try:\n                # Try to get the before and after commits from the GitHub context\n                event_path = os.environ.get(\"GITHUB_EVENT_PATH\")\n                if event_path and os.path.exists(event_path):\n                    with open(event_path, \"r\") as f:\n                        event_data = json.load(f)\n                        before_commit = event_data.get(\"before\")\n                        after_commit = event_data.get(\"after\")\n                        if before_commit and after_commit:\n                            return before_commit, after_commit\n            except Exception as e:\n                print(f\"Error reading GitHub event data: {e}\")\n\n        # Get the first commit in the repository as the \"before\" commit\n        try:\n            # Get the first commit\n            first_commit = None\n            for commit in self.repo.iter_commits(\"--all\", max_parents=0):\n                first_commit = commit.hexsha\n                break  # Just need the first one\n\n            if not first_commit:\n                # Empty repository case - use git's empty tree object\n                first_commit = self.repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n\n            # Use HEAD as the \"after\" commit\n            last_commit = self.repo.head.commit.hexsha\n\n            return first_commit, last_commit\n        except Exception as e:\n            print(f\"Error determining commit range: {e}\")\n            # If all else fails, dynamically create an empty tree object\n            empty_tree = self.repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n            return empty_tree, \"HEAD\"\n\n    def _get_affected_files(\n        self, before_commit: str, after_commit: str\n    ) -> Dict[str, List[str]]:\n        \"\"\"\n        Get files affected (modified, added, deleted) between two commits.\n\n        Args:\n            before_commit: Starting commit hash\n            after_commit: Ending commit hash\n\n        Returns:\n            Dictionary with lists of modified, added, and deleted files\n        \"\"\"\n        result = {\"modified\": [], \"added\": [], \"deleted\": []}\n\n        # Handle first commit case - check if before_commit is an empty tree\n        try:\n            # Try to get the commit - will fail if it's an empty tree\n            self.repo.commit(before_commit)\n        except Exception:\n            # Likely an empty tree - all files are new\n            diff_index = self.repo.git.diff(\n                \"--name-status\", before_commit, after_commit\n            )\n            for line in diff_index.splitlines():\n                parts = line.split(\"\\t\")\n                if len(parts) >= 2:\n                    status, filename = parts[0], parts[1]\n                    if status == \"A\":\n                        file_path = os.path.join(self.repo_path, filename)\n                        result[\"added\"].append(file_path)\n            return result\n\n        # Get diff between commits\n        diff_index = self.repo.git.diff(\"--name-status\", before_commit, after_commit)\n\n        for line in diff_index.splitlines():\n            parts = line.split(\"\\t\")\n            if len(parts) >= 2:\n                status, filename = parts[0], parts[1]\n                file_path = os.path.join(self.repo_path, filename)\n\n                if status == \"M\":\n                    result[\"modified\"].append(file_path)\n                elif status == \"A\":\n                    result[\"added\"].append(file_path)\n                elif status == \"D\":\n                    result[\"deleted\"].append(file_path)\n                elif status.startswith(\"R\"):\n                    # Handle renamed files (old name is deleted, new name is added)\n                    if len(parts) >= 3:\n                        old_filename, new_filename = parts[1], parts[2]\n                        old_path = os.path.join(self.repo_path, old_filename)\n                        new_path = os.path.join(self.repo_path, new_filename)\n                        result[\"deleted\"].append(old_path)\n                        result[\"added\"].append(new_path)\n\n        return result\n\n    def _get_affected_lines(\n        self, file_path: str, before_commit: str, after_commit: str\n    ) -> Set[int]:\n        \"\"\"\n        Get line numbers affected by changes between two commits for a specific file.\n\n        Args:\n            file_path: Path to the file\n            before_commit: Starting commit hash\n            after_commit: Ending commit hash\n\n        Returns:\n            Set of affected line numbers\n        \"\"\"\n        rel_path = os.path.relpath(file_path, self.repo_path)\n        affected_lines = set()\n\n        try:\n            # Get unified diff with context\n            diff_output = self.repo.git.diff(\n                before_commit, after_commit, rel_path, unified=0\n            )\n\n            for line in diff_output.splitlines():\n                if line.startswith(\"@@\"):\n                    # Parse the @@ -a,b +c,d @@ line\n                    parts = line.split()\n                    if len(parts) >= 2:\n                        line_info = parts[1]  # +c,d part\n                        if line_info.startswith(\"+\"):\n                            line_info = line_info[1:]  # Remove the + sign\n                            if \",\" in line_info:\n                                start_line, num_lines = map(int, line_info.split(\",\"))\n                            else:\n                                start_line, num_lines = int(line_info), 1\n\n                            # Add all affected lines to the set\n                            affected_lines.update(\n                                range(start_line, start_line + num_lines)\n                            )\n        except GitCommandError:\n            # File might be new or deleted, consider all lines affected\n            return set(range(1, 100000))\n        except Exception as e:\n            print(f\"Error getting affected lines for {file_path}: {e}\")\n            return set(range(1, 100000))\n\n        return affected_lines\n\n    def _get_file_content_at_commit(\n        self, file_path: str, commit: str\n    ) -> Optional[bytes]:\n        \"\"\"\n        Get the content of a file at a specific commit.\n\n        Args:\n            file_path: Path to the file\n            commit: Commit hash\n\n        Returns:\n            File content as bytes or None if not found\n        \"\"\"\n        rel_path = os.path.relpath(file_path, self.repo_path)\n\n        try:\n            # Get the file content at the specific commit\n            content = self.repo.git.show(f\"{commit}:{rel_path}\")\n            return content.encode(\"utf-8\", errors=\"replace\")\n        except GitCommandError:\n            # File might not exist at this commit\n            return None\n        except Exception as e:\n            print(f\"Error getting file content for {file_path} at {commit}: {e}\")\n            return None\n\n    def _parse_file(\n        self, file_path: str, content: Optional[bytes] = None\n    ) -> List[Dict]:\n        \"\"\"\n        Parse a file to extract code elements (functions, classes, methods).\n\n        Args:\n            file_path: Path to the file\n            content: Optional file content (bytes), reads from file if None\n\n        Returns:\n            List of code elements\n        \"\"\"\n        lang_name = self._get_file_language(file_path)\n        if not lang_name or lang_name not in self.languages:\n            return []\n\n        # Get file content if not provided\n        if content is None:\n            try:\n                with open(file_path, \"rb\") as f:\n                    content = f.read()\n            except Exception as e:\n                print(f\"Error reading file {file_path}: {e}\")\n                return []\n\n        # Define queries for different languages\n        queries = {\n            \"python\": \"\"\"\n                (function_definition\n                  name: (identifier) @function_name) @function\n\n                (class_definition\n                  name: (identifier) @class_name) @class\n            \"\"\",\n            \"javascript\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (class_declaration\n                  name: (identifier) @class_name) @class\n\n                (method_definition\n                  name: (property_identifier) @method_name) @method\n\n                (arrow_function\n                  parameters: (formal_parameters) @params) @arrow_function\n            \"\"\",\n            \"typescript\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (class_declaration\n                  name: (type_identifier) @class_name) @class\n\n                (method_definition\n                  name: (property_identifier) @method_name) @method\n            \"\"\",\n            \"java\": \"\"\"\n                (method_declaration\n                  name: (identifier) @method_name) @method\n\n                (class_declaration\n                  name: (identifier) @class_name) @class\n            \"\"\",\n            \"go\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (method_declaration\n                  name: (field_identifier) @method_name) @method\n            \"\"\",\n            \"rust\": \"\"\"\n                (function_item\n                  name: (identifier) @function_name) @function\n\n                (impl_item\n                  name: (identifier) @impl_name) @impl\n            \"\"\",\n        }\n\n        if lang_name not in queries:\n            return []\n\n        # Parse the file\n        self.parser.language = self.languages[lang_name]\n        tree = self.parser.parse(content)\n\n        print(f\"Parsing file: {file_path}\")\n\n        # Try direct approach using the node iterator\n        code_elements = []\n\n        # Find function definitions\n        for node in self._iter_tree(tree.root_node):\n            element_type = None\n            element_name = None\n\n            # Check node type to determine element type\n            if node.type == \"function_definition\":\n                element_type = \"function\"\n            elif node.type == \"class_definition\":\n                element_type = \"class\"\n            elif node.type == \"method_definition\":\n                element_type = \"method\"\n            elif node.type == \"function_declaration\":\n                element_type = \"function\"\n            elif node.type == \"class_declaration\":\n                element_type = \"class\"\n\n            if element_type:\n                # Find the name node\n                for child in node.children:\n                    if (\n                        child.type == \"identifier\"\n                        or child.type == \"property_identifier\"\n                    ):\n                        element_name = content[\n                            child.start_byte : child.end_byte\n                        ].decode(\"utf-8\", errors=\"replace\")\n                        break\n\n                if element_name:\n                    # Get the full code\n                    element_code = content[node.start_byte : node.end_byte].decode(\n                        \"utf-8\", errors=\"replace\"\n                    )\n\n                    # Use relative path for file_path\n                    rel_file_path = os.path.relpath(file_path, self.repo_path)\n\n                    # Create the element\n                    code_elements.append(\n                        {\n                            \"type\": element_type,\n                            \"name\": element_name,\n                            \"start_line\": node.start_point[0]\n                            + 1,  # 1-based line numbering\n                            \"end_line\": node.end_point[0] + 1,\n                            \"code\": element_code,\n                            \"file_path\": rel_file_path,\n                        }\n                    )\n                    print(f\"Found {element_type}: {element_name}\")\n\n        return code_elements\n\n    def _iter_tree(self, node):\n        \"\"\"Helper method to iterate through all nodes in a tree.\"\"\"\n        yield node\n        for child in node.children:\n            yield from self._iter_tree(child)\n\n    def _parse_file_at_commit(self, file_path: str, commit: str) -> List[Dict]:\n        \"\"\"\n        Parse a file at a specific commit to extract code elements.\n\n        Args:\n            file_path: Path to the file\n            commit: Commit hash\n\n        Returns:\n            List of code elements\n        \"\"\"\n        content = self._get_file_content_at_commit(file_path, commit)\n        if not content:\n            return []\n\n        return self._parse_file(file_path, content)\n\n    def _generate_element_id(self, element: Dict) -> str:\n        \"\"\"\n        Generate a unique ID for a code element.\n\n        Args:\n            element: Code element dictionary\n\n        Returns:\n            Unique ID string\n        \"\"\"\n        rel_path = os.path.relpath(element['file_path'], self.repo_path)\n        unique_str = f\"{rel_path}:{element['name']}:{element['type']}\"\n        return hashlib.md5(unique_str.encode()).hexdigest()\n\n    def analyze_repo_changes(self) -> Dict:\n        \"\"\"\n        Analyze changes in the repository.\n\n        Returns:\n            Dictionary with added, modified, and deleted code elements\n        \"\"\"\n        before_commit, after_commit = self._get_push_commits()\n        print(f\"Analyzing changes between {before_commit} and {after_commit}\")\n\n        # Get affected files\n        affected_files = self._get_affected_files(before_commit, after_commit)\n\n        # Lists to track code elements\n        added_elements = []\n        modified_elements = []\n        deleted_elements = []\n\n        # Process modified files\n        for file_path in affected_files[\"modified\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get affected lines\n            affected_lines = self._get_affected_lines(\n                file_path, before_commit, after_commit\n            )\n\n            # Get elements from before commit\n            before_elements = self._parse_file_at_commit(file_path, before_commit)\n            before_elements_map = {\n                self._generate_element_id(e): e for e in before_elements\n            }\n\n            # Get elements from after commit\n            after_elements = self._parse_file(file_path)\n            after_elements_map = {\n                self._generate_element_id(e): e for e in after_elements\n            }\n\n            # Find added and modified elements\n            for element_id, after_element in after_elements_map.items():\n                # Always consider the element affected for now\n                # This is a simplification to ensure elements are captured\n                is_affected = True\n\n                if element_id in before_elements_map:\n                    # Element exists in both commits - check if modified\n                    before_element = before_elements_map[element_id]\n\n                    # Only add if the code has actually changed\n                    if before_element[\"code\"] != after_element[\"code\"]:\n                        modified_elements.append(after_element)\n\n                        # Update the elements database\n                        self.code_elements_db[\"elements\"][element_id] = {\n                            \"type\": after_element[\"type\"],\n                            \"name\": after_element[\"name\"],\n                            \"file_path\": after_element[\"file_path\"],\n                            \"code\": after_element[\"code\"],\n                            \"last_modified\": after_commit,\n                        }\n                        print(\n                            f\"Modified element: {after_element['type']} {after_element['name']}\"\n                        )\n                else:\n                    # Element exists only in after commit - added\n                    added_elements.append(after_element)\n\n                    # Add to the elements database\n                    self.code_elements_db[\"elements\"][element_id] = {\n                        \"type\": after_element[\"type\"],\n                        \"name\": after_element[\"name\"],\n                        \"file_path\": after_element[\"file_path\"],\n                        \"code\": after_element[\"code\"],\n                        \"added_at\": after_commit,\n                    }\n                    print(\n                        f\"Added element: {after_element['type']} {after_element['name']}\"\n                    )\n\n            # Find deleted elements\n            for element_id, before_element in before_elements_map.items():\n                if element_id not in after_elements_map:\n                    deleted_elements.append(before_element)\n\n                    # Mark as deleted in the database if it exists\n                    if element_id in self.code_elements_db[\"elements\"]:\n                        self.code_elements_db[\"elements\"][element_id][\n                            \"deleted_at\"\n                        ] = after_commit\n                    print(\n                        f\"Deleted element: {before_element['type']} {before_element['name']}\"\n                    )\n\n        # Process added files\n        for file_path in affected_files[\"added\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get elements from the new file\n            elements = self._parse_file(file_path)\n\n            for element in elements:\n                element_id = self._generate_element_id(element)\n                added_elements.append(element)\n\n                # Add to the elements database\n                self.code_elements_db[\"elements\"][element_id] = {\n                    \"type\": element[\"type\"],\n                    \"name\": element[\"name\"],\n                    \"file_path\": element[\"file_path\"],\n                    \"code\": element[\"code\"],\n                    \"added_at\": after_commit,\n                }\n                print(\n                    f\"Added element from new file: {element['type']} {element['name']}\"\n                )\n\n        # Process deleted files\n        for file_path in affected_files[\"deleted\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get elements from the deleted file in the previous commit\n            elements = self._parse_file_at_commit(file_path, before_commit)\n\n            for element in elements:\n                element_id = self._generate_element_id(element)\n                deleted_elements.append(element)\n\n                # Mark as deleted in the database if it exists\n                if element_id in self.code_elements_db[\"elements\"]:\n                    self.code_elements_db[\"elements\"][element_id][\n                        \"deleted_at\"\n                    ] = after_commit\n                print(\n                    f\"Deleted element from removed file: {element['type']} {element['name']}\"\n                )\n\n        # Update metadata\n        self.code_elements_db[\"metadata\"][\"last_processed_commit\"] = after_commit\n\n        # Save the updated database\n        self._save_code_elements_db()\n\n        # Generate report JSON\n        report = {\n            \"affected_elements\": {\n                \"added\": added_elements,\n                \"modified\": modified_elements,\n                \"deleted\": deleted_elements,\n            },\n            \"stats\": {\n                \"added\": len(added_elements),\n                \"modified\": len(modified_elements),\n                \"deleted\": len(deleted_elements),\n                \"total_elements\": len(self.code_elements_db[\"elements\"]),\n            },\n            \"metadata\": {\n                \"repository\": os.path.basename(self.repo_path),\n                \"commit_range\": {\"before\": before_commit, \"after\": after_commit},\n                \"timestamp\": self.repo.head.commit.committed_datetime.isoformat(),\n            },\n        }\n\n        # Save the report\n        report_path = os.path.join(DOCAI_DIR, f\"change_report_{after_commit[:7]}.json\")\n        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(report, f, indent=2)\n\n        print(f\"Report saved to {report_path}\")\n        print(\n            f\"Added: {len(added_elements)}, Modified: {len(modified_elements)}, Deleted: {len(deleted_elements)}\"\n        )\n\n        return report",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "start_line": 68,
        "end_line": 80,
        "code": "def __init__(self, repo_path: str):\n        \"\"\"\n        Initialize the CodeAnalyzer.\n\n        Args:\n            repo_path: Path to the git repository\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = Repo(repo_path)\n        self.languages = self._setup_tree_sitter()\n        self.parser = Parser()\n        self.code_elements_db = self._load_code_elements_db()\n        self._custom_commit_range = None",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_setup_tree_sitter",
        "start_line": 82,
        "end_line": 91,
        "code": "def _setup_tree_sitter(self) -> Dict[str, Language]:\n        \"\"\"Initialize and load tree-sitter languages.\"\"\"\n        lang_objects = {}\n        for lang_name, lang_module in LANGUAGE_MODULES.items():\n            try:\n                if lang_module():\n                    lang_objects[lang_name] = Language(lang_module())\n            except Exception as e:\n                print(f\"Error loading language {lang_name}: {e}\")\n        return lang_objects",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_load_code_elements_db",
        "start_line": 93,
        "end_line": 105,
        "code": "def _load_code_elements_db(self) -> Dict:\n        \"\"\"Load existing code elements database if it exists.\"\"\"\n        if os.path.exists(ELEMENTS_DB_PATH):\n            try:\n                with open(ELEMENTS_DB_PATH, \"r\", encoding=\"utf-8\") as f:\n                    return json.load(f)\n            except json.JSONDecodeError:\n                print(\n                    f\"Warning: Invalid JSON in {ELEMENTS_DB_PATH}, creating new database\"\n                )\n\n        # Return empty database structure if file doesn't exist or is invalid\n        return {\"elements\": {}, \"metadata\": {\"last_processed_commit\": None}}",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_save_code_elements_db",
        "start_line": 107,
        "end_line": 110,
        "code": "def _save_code_elements_db(self):\n        \"\"\"Save the code elements database.\"\"\"\n        with open(ELEMENTS_DB_PATH, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.code_elements_db, f, indent=2)",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_get_file_language",
        "start_line": 112,
        "end_line": 123,
        "code": "def _get_file_language(self, file_path: str) -> Optional[str]:\n        \"\"\"\n        Determine the programming language of a file based on its extension.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            Language name or None if not supported\n        \"\"\"\n        extension = os.path.splitext(file_path)[1].lower()\n        return LANGUAGE_EXTENSIONS.get(extension)",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_get_push_commits",
        "start_line": 125,
        "end_line": 175,
        "code": "def _get_push_commits(self) -> Tuple[str, str]:\n        \"\"\"\n        Get the commit range for the current push.\n\n        For GitHub Actions, determines the before and after commits of the push.\n        Can be overridden by command-line arguments.\n\n        Returns:\n            Tuple of (before_commit, after_commit)\n        \"\"\"\n        # If custom commit range is provided, use it\n        if self._custom_commit_range:\n            return self._custom_commit_range\n\n        # In GitHub Actions environment\n        if os.environ.get(\"GITHUB_EVENT_NAME\") == \"push\":\n            # If this is a push event, get the before and after SHAs\n            try:\n                # Try to get the before and after commits from the GitHub context\n                event_path = os.environ.get(\"GITHUB_EVENT_PATH\")\n                if event_path and os.path.exists(event_path):\n                    with open(event_path, \"r\") as f:\n                        event_data = json.load(f)\n                        before_commit = event_data.get(\"before\")\n                        after_commit = event_data.get(\"after\")\n                        if before_commit and after_commit:\n                            return before_commit, after_commit\n            except Exception as e:\n                print(f\"Error reading GitHub event data: {e}\")\n\n        # Get the first commit in the repository as the \"before\" commit\n        try:\n            # Get the first commit\n            first_commit = None\n            for commit in self.repo.iter_commits(\"--all\", max_parents=0):\n                first_commit = commit.hexsha\n                break  # Just need the first one\n\n            if not first_commit:\n                # Empty repository case - use git's empty tree object\n                first_commit = self.repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n\n            # Use HEAD as the \"after\" commit\n            last_commit = self.repo.head.commit.hexsha\n\n            return first_commit, last_commit\n        except Exception as e:\n            print(f\"Error determining commit range: {e}\")\n            # If all else fails, dynamically create an empty tree object\n            empty_tree = self.repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n            return empty_tree, \"HEAD\"",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_get_affected_files",
        "start_line": 177,
        "end_line": 234,
        "code": "def _get_affected_files(\n        self, before_commit: str, after_commit: str\n    ) -> Dict[str, List[str]]:\n        \"\"\"\n        Get files affected (modified, added, deleted) between two commits.\n\n        Args:\n            before_commit: Starting commit hash\n            after_commit: Ending commit hash\n\n        Returns:\n            Dictionary with lists of modified, added, and deleted files\n        \"\"\"\n        result = {\"modified\": [], \"added\": [], \"deleted\": []}\n\n        # Handle first commit case - check if before_commit is an empty tree\n        try:\n            # Try to get the commit - will fail if it's an empty tree\n            self.repo.commit(before_commit)\n        except Exception:\n            # Likely an empty tree - all files are new\n            diff_index = self.repo.git.diff(\n                \"--name-status\", before_commit, after_commit\n            )\n            for line in diff_index.splitlines():\n                parts = line.split(\"\\t\")\n                if len(parts) >= 2:\n                    status, filename = parts[0], parts[1]\n                    if status == \"A\":\n                        file_path = os.path.join(self.repo_path, filename)\n                        result[\"added\"].append(file_path)\n            return result\n\n        # Get diff between commits\n        diff_index = self.repo.git.diff(\"--name-status\", before_commit, after_commit)\n\n        for line in diff_index.splitlines():\n            parts = line.split(\"\\t\")\n            if len(parts) >= 2:\n                status, filename = parts[0], parts[1]\n                file_path = os.path.join(self.repo_path, filename)\n\n                if status == \"M\":\n                    result[\"modified\"].append(file_path)\n                elif status == \"A\":\n                    result[\"added\"].append(file_path)\n                elif status == \"D\":\n                    result[\"deleted\"].append(file_path)\n                elif status.startswith(\"R\"):\n                    # Handle renamed files (old name is deleted, new name is added)\n                    if len(parts) >= 3:\n                        old_filename, new_filename = parts[1], parts[2]\n                        old_path = os.path.join(self.repo_path, old_filename)\n                        new_path = os.path.join(self.repo_path, new_filename)\n                        result[\"deleted\"].append(old_path)\n                        result[\"added\"].append(new_path)\n\n        return result",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_get_affected_lines",
        "start_line": 236,
        "end_line": 283,
        "code": "def _get_affected_lines(\n        self, file_path: str, before_commit: str, after_commit: str\n    ) -> Set[int]:\n        \"\"\"\n        Get line numbers affected by changes between two commits for a specific file.\n\n        Args:\n            file_path: Path to the file\n            before_commit: Starting commit hash\n            after_commit: Ending commit hash\n\n        Returns:\n            Set of affected line numbers\n        \"\"\"\n        rel_path = os.path.relpath(file_path, self.repo_path)\n        affected_lines = set()\n\n        try:\n            # Get unified diff with context\n            diff_output = self.repo.git.diff(\n                before_commit, after_commit, rel_path, unified=0\n            )\n\n            for line in diff_output.splitlines():\n                if line.startswith(\"@@\"):\n                    # Parse the @@ -a,b +c,d @@ line\n                    parts = line.split()\n                    if len(parts) >= 2:\n                        line_info = parts[1]  # +c,d part\n                        if line_info.startswith(\"+\"):\n                            line_info = line_info[1:]  # Remove the + sign\n                            if \",\" in line_info:\n                                start_line, num_lines = map(int, line_info.split(\",\"))\n                            else:\n                                start_line, num_lines = int(line_info), 1\n\n                            # Add all affected lines to the set\n                            affected_lines.update(\n                                range(start_line, start_line + num_lines)\n                            )\n        except GitCommandError:\n            # File might be new or deleted, consider all lines affected\n            return set(range(1, 100000))\n        except Exception as e:\n            print(f\"Error getting affected lines for {file_path}: {e}\")\n            return set(range(1, 100000))\n\n        return affected_lines",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_get_file_content_at_commit",
        "start_line": 285,
        "end_line": 309,
        "code": "def _get_file_content_at_commit(\n        self, file_path: str, commit: str\n    ) -> Optional[bytes]:\n        \"\"\"\n        Get the content of a file at a specific commit.\n\n        Args:\n            file_path: Path to the file\n            commit: Commit hash\n\n        Returns:\n            File content as bytes or None if not found\n        \"\"\"\n        rel_path = os.path.relpath(file_path, self.repo_path)\n\n        try:\n            # Get the file content at the specific commit\n            content = self.repo.git.show(f\"{commit}:{rel_path}\")\n            return content.encode(\"utf-8\", errors=\"replace\")\n        except GitCommandError:\n            # File might not exist at this commit\n            return None\n        except Exception as e:\n            print(f\"Error getting file content for {file_path} at {commit}: {e}\")\n            return None",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_parse_file",
        "start_line": 311,
        "end_line": 456,
        "code": "def _parse_file(\n        self, file_path: str, content: Optional[bytes] = None\n    ) -> List[Dict]:\n        \"\"\"\n        Parse a file to extract code elements (functions, classes, methods).\n\n        Args:\n            file_path: Path to the file\n            content: Optional file content (bytes), reads from file if None\n\n        Returns:\n            List of code elements\n        \"\"\"\n        lang_name = self._get_file_language(file_path)\n        if not lang_name or lang_name not in self.languages:\n            return []\n\n        # Get file content if not provided\n        if content is None:\n            try:\n                with open(file_path, \"rb\") as f:\n                    content = f.read()\n            except Exception as e:\n                print(f\"Error reading file {file_path}: {e}\")\n                return []\n\n        # Define queries for different languages\n        queries = {\n            \"python\": \"\"\"\n                (function_definition\n                  name: (identifier) @function_name) @function\n\n                (class_definition\n                  name: (identifier) @class_name) @class\n            \"\"\",\n            \"javascript\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (class_declaration\n                  name: (identifier) @class_name) @class\n\n                (method_definition\n                  name: (property_identifier) @method_name) @method\n\n                (arrow_function\n                  parameters: (formal_parameters) @params) @arrow_function\n            \"\"\",\n            \"typescript\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (class_declaration\n                  name: (type_identifier) @class_name) @class\n\n                (method_definition\n                  name: (property_identifier) @method_name) @method\n            \"\"\",\n            \"java\": \"\"\"\n                (method_declaration\n                  name: (identifier) @method_name) @method\n\n                (class_declaration\n                  name: (identifier) @class_name) @class\n            \"\"\",\n            \"go\": \"\"\"\n                (function_declaration\n                  name: (identifier) @function_name) @function\n\n                (method_declaration\n                  name: (field_identifier) @method_name) @method\n            \"\"\",\n            \"rust\": \"\"\"\n                (function_item\n                  name: (identifier) @function_name) @function\n\n                (impl_item\n                  name: (identifier) @impl_name) @impl\n            \"\"\",\n        }\n\n        if lang_name not in queries:\n            return []\n\n        # Parse the file\n        self.parser.language = self.languages[lang_name]\n        tree = self.parser.parse(content)\n\n        print(f\"Parsing file: {file_path}\")\n\n        # Try direct approach using the node iterator\n        code_elements = []\n\n        # Find function definitions\n        for node in self._iter_tree(tree.root_node):\n            element_type = None\n            element_name = None\n\n            # Check node type to determine element type\n            if node.type == \"function_definition\":\n                element_type = \"function\"\n            elif node.type == \"class_definition\":\n                element_type = \"class\"\n            elif node.type == \"method_definition\":\n                element_type = \"method\"\n            elif node.type == \"function_declaration\":\n                element_type = \"function\"\n            elif node.type == \"class_declaration\":\n                element_type = \"class\"\n\n            if element_type:\n                # Find the name node\n                for child in node.children:\n                    if (\n                        child.type == \"identifier\"\n                        or child.type == \"property_identifier\"\n                    ):\n                        element_name = content[\n                            child.start_byte : child.end_byte\n                        ].decode(\"utf-8\", errors=\"replace\")\n                        break\n\n                if element_name:\n                    # Get the full code\n                    element_code = content[node.start_byte : node.end_byte].decode(\n                        \"utf-8\", errors=\"replace\"\n                    )\n\n                    # Use relative path for file_path\n                    rel_file_path = os.path.relpath(file_path, self.repo_path)\n\n                    # Create the element\n                    code_elements.append(\n                        {\n                            \"type\": element_type,\n                            \"name\": element_name,\n                            \"start_line\": node.start_point[0]\n                            + 1,  # 1-based line numbering\n                            \"end_line\": node.end_point[0] + 1,\n                            \"code\": element_code,\n                            \"file_path\": rel_file_path,\n                        }\n                    )\n                    print(f\"Found {element_type}: {element_name}\")\n\n        return code_elements",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_iter_tree",
        "start_line": 458,
        "end_line": 462,
        "code": "def _iter_tree(self, node):\n        \"\"\"Helper method to iterate through all nodes in a tree.\"\"\"\n        yield node\n        for child in node.children:\n            yield from self._iter_tree(child)",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_parse_file_at_commit",
        "start_line": 464,
        "end_line": 479,
        "code": "def _parse_file_at_commit(self, file_path: str, commit: str) -> List[Dict]:\n        \"\"\"\n        Parse a file at a specific commit to extract code elements.\n\n        Args:\n            file_path: Path to the file\n            commit: Commit hash\n\n        Returns:\n            List of code elements\n        \"\"\"\n        content = self._get_file_content_at_commit(file_path, commit)\n        if not content:\n            return []\n\n        return self._parse_file(file_path, content)",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "_generate_element_id",
        "start_line": 481,
        "end_line": 493,
        "code": "def _generate_element_id(self, element: Dict) -> str:\n        \"\"\"\n        Generate a unique ID for a code element.\n\n        Args:\n            element: Code element dictionary\n\n        Returns:\n            Unique ID string\n        \"\"\"\n        rel_path = os.path.relpath(element['file_path'], self.repo_path)\n        unique_str = f\"{rel_path}:{element['name']}:{element['type']}\"\n        return hashlib.md5(unique_str.encode()).hexdigest()",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "analyze_repo_changes",
        "start_line": 495,
        "end_line": 674,
        "code": "def analyze_repo_changes(self) -> Dict:\n        \"\"\"\n        Analyze changes in the repository.\n\n        Returns:\n            Dictionary with added, modified, and deleted code elements\n        \"\"\"\n        before_commit, after_commit = self._get_push_commits()\n        print(f\"Analyzing changes between {before_commit} and {after_commit}\")\n\n        # Get affected files\n        affected_files = self._get_affected_files(before_commit, after_commit)\n\n        # Lists to track code elements\n        added_elements = []\n        modified_elements = []\n        deleted_elements = []\n\n        # Process modified files\n        for file_path in affected_files[\"modified\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get affected lines\n            affected_lines = self._get_affected_lines(\n                file_path, before_commit, after_commit\n            )\n\n            # Get elements from before commit\n            before_elements = self._parse_file_at_commit(file_path, before_commit)\n            before_elements_map = {\n                self._generate_element_id(e): e for e in before_elements\n            }\n\n            # Get elements from after commit\n            after_elements = self._parse_file(file_path)\n            after_elements_map = {\n                self._generate_element_id(e): e for e in after_elements\n            }\n\n            # Find added and modified elements\n            for element_id, after_element in after_elements_map.items():\n                # Always consider the element affected for now\n                # This is a simplification to ensure elements are captured\n                is_affected = True\n\n                if element_id in before_elements_map:\n                    # Element exists in both commits - check if modified\n                    before_element = before_elements_map[element_id]\n\n                    # Only add if the code has actually changed\n                    if before_element[\"code\"] != after_element[\"code\"]:\n                        modified_elements.append(after_element)\n\n                        # Update the elements database\n                        self.code_elements_db[\"elements\"][element_id] = {\n                            \"type\": after_element[\"type\"],\n                            \"name\": after_element[\"name\"],\n                            \"file_path\": after_element[\"file_path\"],\n                            \"code\": after_element[\"code\"],\n                            \"last_modified\": after_commit,\n                        }\n                        print(\n                            f\"Modified element: {after_element['type']} {after_element['name']}\"\n                        )\n                else:\n                    # Element exists only in after commit - added\n                    added_elements.append(after_element)\n\n                    # Add to the elements database\n                    self.code_elements_db[\"elements\"][element_id] = {\n                        \"type\": after_element[\"type\"],\n                        \"name\": after_element[\"name\"],\n                        \"file_path\": after_element[\"file_path\"],\n                        \"code\": after_element[\"code\"],\n                        \"added_at\": after_commit,\n                    }\n                    print(\n                        f\"Added element: {after_element['type']} {after_element['name']}\"\n                    )\n\n            # Find deleted elements\n            for element_id, before_element in before_elements_map.items():\n                if element_id not in after_elements_map:\n                    deleted_elements.append(before_element)\n\n                    # Mark as deleted in the database if it exists\n                    if element_id in self.code_elements_db[\"elements\"]:\n                        self.code_elements_db[\"elements\"][element_id][\n                            \"deleted_at\"\n                        ] = after_commit\n                    print(\n                        f\"Deleted element: {before_element['type']} {before_element['name']}\"\n                    )\n\n        # Process added files\n        for file_path in affected_files[\"added\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get elements from the new file\n            elements = self._parse_file(file_path)\n\n            for element in elements:\n                element_id = self._generate_element_id(element)\n                added_elements.append(element)\n\n                # Add to the elements database\n                self.code_elements_db[\"elements\"][element_id] = {\n                    \"type\": element[\"type\"],\n                    \"name\": element[\"name\"],\n                    \"file_path\": element[\"file_path\"],\n                    \"code\": element[\"code\"],\n                    \"added_at\": after_commit,\n                }\n                print(\n                    f\"Added element from new file: {element['type']} {element['name']}\"\n                )\n\n        # Process deleted files\n        for file_path in affected_files[\"deleted\"]:\n            lang_name = self._get_file_language(file_path)\n            if not lang_name or lang_name not in self.languages:\n                continue\n\n            # Get elements from the deleted file in the previous commit\n            elements = self._parse_file_at_commit(file_path, before_commit)\n\n            for element in elements:\n                element_id = self._generate_element_id(element)\n                deleted_elements.append(element)\n\n                # Mark as deleted in the database if it exists\n                if element_id in self.code_elements_db[\"elements\"]:\n                    self.code_elements_db[\"elements\"][element_id][\n                        \"deleted_at\"\n                    ] = after_commit\n                print(\n                    f\"Deleted element from removed file: {element['type']} {element['name']}\"\n                )\n\n        # Update metadata\n        self.code_elements_db[\"metadata\"][\"last_processed_commit\"] = after_commit\n\n        # Save the updated database\n        self._save_code_elements_db()\n\n        # Generate report JSON\n        report = {\n            \"affected_elements\": {\n                \"added\": added_elements,\n                \"modified\": modified_elements,\n                \"deleted\": deleted_elements,\n            },\n            \"stats\": {\n                \"added\": len(added_elements),\n                \"modified\": len(modified_elements),\n                \"deleted\": len(deleted_elements),\n                \"total_elements\": len(self.code_elements_db[\"elements\"]),\n            },\n            \"metadata\": {\n                \"repository\": os.path.basename(self.repo_path),\n                \"commit_range\": {\"before\": before_commit, \"after\": after_commit},\n                \"timestamp\": self.repo.head.commit.committed_datetime.isoformat(),\n            },\n        }\n\n        # Save the report\n        report_path = os.path.join(DOCAI_DIR, f\"change_report_{after_commit[:7]}.json\")\n        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(report, f, indent=2)\n\n        print(f\"Report saved to {report_path}\")\n        print(\n            f\"Added: {len(added_elements)}, Modified: {len(modified_elements)}, Deleted: {len(deleted_elements)}\"\n        )\n\n        return report",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "function",
        "name": "main",
        "start_line": 677,
        "end_line": 734,
        "code": "def main():\n    try:\n        # Set up argument parser\n        parser = argparse.ArgumentParser(\n            description=\"Analyze code changes between commits\"\n        )\n        parser.add_argument(\n            \"--before\", help=\"Starting commit hash (defaults to first commit in repo)\"\n        )\n        parser.add_argument(\"--after\", help=\"Ending commit hash (defaults to HEAD)\")\n        args = parser.parse_args()\n\n        print(\"Starting code analysis...\")\n        analyzer = CodeAnalyzer(REPO_PATH)\n\n        # If commit hashes are provided via command line, use them\n        if args.before is not None or args.after is not None:\n            repo = Repo(REPO_PATH)\n\n            # Handle before commit\n            if args.before is None:\n                # If before is not provided, use the first commit in the repository\n                try:\n                    # Find commits with no parents (root commits)\n                    first_commit = None\n                    for commit in repo.iter_commits(\"--all\", max_parents=0):\n                        first_commit = commit.hexsha\n                        break  # Just need the first one\n\n                    if first_commit:\n                        before_commit = first_commit\n                        print(f\"Using first commit as before: {before_commit}\")\n                    else:\n                        # Empty repository case - use git's empty tree object\n                        before_commit = repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n                except Exception as e:\n                    print(f\"Error finding first commit: {e}\")\n                    before_commit = repo.git.hash_object(\"-t\", \"tree\", \"/dev/null\")\n            else:\n                before_commit = args.before\n\n            # Handle after commit\n            if args.after is None:\n                after_commit = \"HEAD\"\n            else:\n                after_commit = args.after\n\n            # Override the commit range\n            analyzer._custom_commit_range = (before_commit, after_commit)\n\n        analyzer.analyze_repo_changes()\n        print(\"Code analysis completed successfully\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n        import traceback\n\n        traceback.print_exc()\n        sys.exit(1)",
        "file_path": "analyze_code_changes.py"
      },
      {
        "type": "class",
        "name": "ApiConfig",
        "start_line": 6,
        "end_line": 8,
        "code": "class ApiConfig(AppConfig):\n    default_auto_field = \"django.db.models.BigAutoField\"\n    name = \"api\"",
        "file_path": "api/apps.py"
      },
      {
        "type": "function",
        "name": "main",
        "start_line": 9,
        "end_line": 20,
        "code": "def main():\n    \"\"\"Run administrative tasks.\"\"\"\n    os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"docAI.settings\")\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            \"Couldn't import Django. Are you sure it's installed and \"\n            \"available on your PYTHONPATH environment variable? Did you \"\n            \"forget to activate a virtual environment?\",\n        ) from exc\n    execute_from_command_line(sys.argv)",
        "file_path": "manage.py"
      },
      {
        "type": "class",
        "name": "CodeElement",
        "start_line": 11,
        "end_line": 20,
        "code": "class CodeElement:\n    name: str\n    type: str  # 'function', 'class', 'method'\n    path: str\n    code: str\n    docstring: Optional[str]\n    dependencies: Set[str]\n    description: Optional[str] = None\n    analyzed: bool = False\n    parent_class: Optional[str] = None",
        "file_path": "parser/parser.py"
      },
      {
        "type": "class",
        "name": "CodeAnalyzer",
        "start_line": 23,
        "end_line": 329,
        "code": "class CodeAnalyzer:\n    def __init__(self, root_path: str):\n        PY_LANGUAGE = Language(tspython.language())\n        self.parser = Parser(PY_LANGUAGE)\n        self.elements: Dict[str, CodeElement] = {}\n        self.dependency_graph = nx.DiGraph()\n\n    def parse_files(self, directory_path: str):\n        \"\"\"Parse all Python files in the directory.\"\"\"\n        for root, _, files in os.walk(directory_path):\n            for file in files:\n                if file.endswith(\".py\"):\n                    file_path = os.path.join(root, file)\n                    self._parse_file(file_path)\n\n    def _parse_file(self, file_path: str):\n        \"\"\"Parse a single file and extract code elements.\"\"\"\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n            tree = self.parser.parse(bytes(content, \"utf8\"))\n            self._extract_elements(tree, content, file_path)\n\n    def _extract_elements(self, tree, content: str, file_path: str):\n        \"\"\"Extract functions, classes, and their dependencies.\"\"\"\n        # Query for functions and classes\n        query_string = \"\"\"\n            (function_definition\n                name: (identifier) @function.name\n                body: (block)? @function.body) @function.def\n\n            (class_definition\n                name: (identifier) @class.name\n                body: (block)? @class.body) @class.def\n\n            (call function: (_) @call.name)\n        \"\"\"\n        query = self.parser.language.query(query_string)\n        captures = query.captures(tree.root_node)\n        print(type(captures))\n        # for capture_type, node in captures:\n        #     print(\"Ctype:\",capture_type)\n        # for cap in captures:\n        #     print(cap)\n\n        current_class = None\n\n        for capture_type, node_list in captures.items():\n            for node in node_list:\n                if capture_type == \"function.name\":\n                    func_node = node.parent\n                    name = content[node.start_byte : node.end_byte]\n                    # fmt: off\n                    full_name = (f\"{current_class}.{name}\" if current_class else name )\n                    # fmt: on\n\n                    # Extract docstring if exists\n                    docstring = self._extract_docstring(func_node, content)\n\n                    # Get function code\n                    code = content[func_node.start_byte : func_node.end_byte]\n\n                    # Find dependencies (function calls)\n                    dependencies = self._find_dependencies(func_node, content)\n\n                    element = CodeElement(\n                        name=full_name,\n                        path=file_path,\n                        type=\"method\" if current_class else \"function\",\n                        code=code,\n                        docstring=docstring,\n                        dependencies=dependencies,\n                        parent_class=current_class,\n                    )\n                    self.elements[full_name] = element\n\n                elif capture_type == \"class.name\":\n                    current_class = content[node.start_byte : node.end_byte]\n                    class_node = node.parent\n\n                    # Extract class docstring\n                    docstring = self._extract_docstring(class_node, content)\n\n                    # Get class code\n                    code = content[class_node.start_byte : class_node.end_byte]\n\n                    # Find class dependencies\n                    dependencies = self._find_dependencies(class_node, content)\n\n                    element = CodeElement(\n                        name=current_class,\n                        type=\"class\",\n                        path=file_path,\n                        code=code,\n                        docstring=docstring,\n                        dependencies=dependencies,\n                    )\n                    self.elements[current_class] = element\n\n    def _extract_docstring(self, node, content: str) -> Optional[str]:\n        \"\"\"Extract docstring from a function or class node.\"\"\"\n        for child in node.children:\n            if child.type == \"block\":\n                for block_child in child.children:\n                    if block_child.type == \"expression_statement\":\n                        expr = block_child.children[0]\n                        if expr.type == \"string\":\n                            return content[expr.start_byte : expr.end_byte].strip(\n                                \"\\\"' \\n\"\n                            )\n        return None\n\n    def _find_dependencies(self, node, content: str) -> Set[str]:\n        \"\"\"Find all function/class dependencies in a node.\"\"\"\n        dependencies = set()\n\n        def visit_node(node):\n            if node.type == \"call\":\n                func_name = content[\n                    node.child_by_field_name(\"function\")\n                    .start_byte : node.child_by_field_name(\"function\")\n                    .end_byte\n                ]\n                dependencies.add(func_name)\n            for child in node.children:\n                visit_node(child)\n\n        visit_node(node)\n        return dependencies\n\n    def build_dependency_graph(self):\n        \"\"\"Build a directed graph of dependencies.\"\"\"\n        self.dependency_graph.clear()\n\n        # Add nodes\n        for name, element in self.elements.items():\n            self.dependency_graph.add_node(name, **element.__dict__)\n\n        # Add edges\n        for name, element in self.elements.items():\n            for dep in element.dependencies:\n                if dep in self.elements:\n                    self.dependency_graph.add_edge(name, dep)\n\n    def analyze_code_hierarchically(self):\n        \"\"\"Analyze code elements from lowest to highest dependencies.\"\"\"\n        self.build_dependency_graph()\n\n        # Get elements in topological order (from least to most dependent)\n        try:\n            ordered_elements = list(nx.topological_sort(self.dependency_graph))\n        except nx.NetworkXUnfeasible:\n            print(\n                \"Warning: Circular dependencies detected. Using approximate ordering.\"\n            )\n            ordered_elements = list(self.elements.keys())\n\n        # Analyze each element\n        for element_name in ordered_elements:\n            element = self.elements[element_name]\n            if not element.analyzed:\n                self._analyze_element(element)\n\n    def _analyze_element(self, element: CodeElement):\n        \"\"\"Generate analysis and description for a code element.\"\"\"\n        # Collect information about dependencies\n        dep_descriptions = []\n        for dep in element.dependencies:\n            if dep in self.elements:\n                dep_element = self.elements[dep]\n                if dep_element.description:\n                    dep_descriptions.append(f\"{dep}: {dep_element.description}\")\n\n        # This is where you'd normally call an LLM. For demo, we'll generate a simple description\n        description_parts = [\n            f\"Type: {element.type}\",\n            f\"Dependencies: {len(element.dependencies)}\",\n        ]\n\n        if element.docstring:\n            description_parts.append(f\"Documentation: {element.docstring}\")\n\n        if dep_descriptions:\n            description_parts.append(\"Uses:\")\n            description_parts.extend(f\"- {desc}\" for desc in dep_descriptions)\n\n        element.description = \"\\n\".join(description_parts)\n        element.analyzed = True\n\n    # def generate_report(self) -> dict:\n    #     \"\"\"Generate a hierarchical report of the codebase.\"\"\"\n    #     self.analyze_code_hierarchically()\n\n    #     report = {\n    #         \"elements\": {},\n    #         \"dependency_tree\": {},\n    #         \"statistics\": {\n    #             \"total_elements\": len(self.elements),\n    #             \"functions\": len([e for e in self.elements.values() if e.type == 'function']),\n    #             \"classes\": len([e for e in self.elements.values() if e.type == 'class']),\n    #             \"methods\": len([e for e in self.elements.values() if e.type == 'method'])\n    #         }\n    #     }\n\n    #     # Build hierarchical structure\n    #     def build_tree(element_name):\n    #         element = self.elements[element_name]\n    #         return {\n    #             \"type\": element.type,\n    #             \"description\": element.description,\n    #             \"dependencies\": {dep: build_tree(dep) for dep in element.dependencies if dep in self.elements}\n    #         }\n\n    #     # Add each top-level element to the report\n    #     for name, element in self.elements.items():\n    #         report[\"elements\"][name] = {\n    #             \"type\": element.type,\n    #             \"description\": element.description,\n    #             \"code\": element.code,\n    #             \"dependencies\": list(element.dependencies)\n    #         }\n\n    #         # Only build trees for top-level elements\n    #         if not element.parent_class:\n    #             report[\"dependency_tree\"][name] = build_tree(name)\n\n    #     return report\n\n    def generate_report(self) -> dict:\n        \"\"\"Generate a hierarchical report of the codebase.\"\"\"\n        self.analyze_code_hierarchically()\n\n        report = {\n            \"elements\": {},\n            \"dependency_tree\": {},\n            \"statistics\": {\n                \"total_elements\": len(self.elements),\n                \"functions\": len(\n                    [e for e in self.elements.values() if e.type == \"function\"]\n                ),\n                \"classes\": len(\n                    [e for e in self.elements.values() if e.type == \"class\"]\n                ),\n                \"methods\": len(\n                    [e for e in self.elements.values() if e.type == \"method\"]\n                ),\n            },\n        }\n\n        def build_tree(element_name, visited=None):\n            \"\"\"Build dependency tree with cycle detection.\"\"\"\n            if visited is None:\n                visited = set()\n\n            if element_name in visited:\n                return {\"type\": \"circular_reference\", \"name\": element_name}\n\n            if element_name not in self.elements:\n                return {\"type\": \"unknown_reference\", \"name\": element_name}\n\n            visited.add(element_name)\n            element = self.elements[element_name]\n\n            tree = {\n                \"type\": element.type,\n                \"name\": element_name,\n                \"path\": element.path,\n                \"description\": element.description,\n                \"dependencies\": {},\n            }\n\n            for dep in element.dependencies:\n                if dep in self.elements:\n                    tree[\"dependencies\"][dep] = build_tree(dep, visited.copy())\n\n            return tree\n\n        # Add each top-level element to the report\n        for name, element in self.elements.items():\n            report[\"elements\"][name] = {\n                \"type\": element.type,\n                \"path\": element.path,\n                \"description\": element.description,\n                \"code\": element.code,\n                \"dependencies\": list(element.dependencies),\n            }\n\n            # Only build trees for top-level elements\n            if not element.parent_class:\n                report[\"dependency_tree\"][name] = build_tree(name)\n\n        # Add cycles information to the report\n        cycles = self._find_cycles()\n        if cycles:\n            report[\"cycles\"] = list(cycles)\n\n        return report\n\n    def _find_cycles(self) -> Set[tuple]:\n        \"\"\"Find and return all dependency cycles in the graph.\"\"\"\n        cycles = set()\n        try:\n            cycles = set(\n                tuple(cycle) for cycle in nx.simple_cycles(self.dependency_graph)\n            )\n        except Exception as e:\n            print(f\"Warning: Error detecting cycles: {str(e)}\")\n        return cycles",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "start_line": 24,
        "end_line": 28,
        "code": "def __init__(self, root_path: str):\n        PY_LANGUAGE = Language(tspython.language())\n        self.parser = Parser(PY_LANGUAGE)\n        self.elements: Dict[str, CodeElement] = {}\n        self.dependency_graph = nx.DiGraph()",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "parse_files",
        "start_line": 30,
        "end_line": 36,
        "code": "def parse_files(self, directory_path: str):\n        \"\"\"Parse all Python files in the directory.\"\"\"\n        for root, _, files in os.walk(directory_path):\n            for file in files:\n                if file.endswith(\".py\"):\n                    file_path = os.path.join(root, file)\n                    self._parse_file(file_path)",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "_parse_file",
        "start_line": 38,
        "end_line": 43,
        "code": "def _parse_file(self, file_path: str):\n        \"\"\"Parse a single file and extract code elements.\"\"\"\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n            tree = self.parser.parse(bytes(content, \"utf8\"))\n            self._extract_elements(tree, content, file_path)",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "_extract_elements",
        "start_line": 45,
        "end_line": 119,
        "code": "def _extract_elements(self, tree, content: str, file_path: str):\n        \"\"\"Extract functions, classes, and their dependencies.\"\"\"\n        # Query for functions and classes\n        query_string = \"\"\"\n            (function_definition\n                name: (identifier) @function.name\n                body: (block)? @function.body) @function.def\n\n            (class_definition\n                name: (identifier) @class.name\n                body: (block)? @class.body) @class.def\n\n            (call function: (_) @call.name)\n        \"\"\"\n        query = self.parser.language.query(query_string)\n        captures = query.captures(tree.root_node)\n        print(type(captures))\n        # for capture_type, node in captures:\n        #     print(\"Ctype:\",capture_type)\n        # for cap in captures:\n        #     print(cap)\n\n        current_class = None\n\n        for capture_type, node_list in captures.items():\n            for node in node_list:\n                if capture_type == \"function.name\":\n                    func_node = node.parent\n                    name = content[node.start_byte : node.end_byte]\n                    # fmt: off\n                    full_name = (f\"{current_class}.{name}\" if current_class else name )\n                    # fmt: on\n\n                    # Extract docstring if exists\n                    docstring = self._extract_docstring(func_node, content)\n\n                    # Get function code\n                    code = content[func_node.start_byte : func_node.end_byte]\n\n                    # Find dependencies (function calls)\n                    dependencies = self._find_dependencies(func_node, content)\n\n                    element = CodeElement(\n                        name=full_name,\n                        path=file_path,\n                        type=\"method\" if current_class else \"function\",\n                        code=code,\n                        docstring=docstring,\n                        dependencies=dependencies,\n                        parent_class=current_class,\n                    )\n                    self.elements[full_name] = element\n\n                elif capture_type == \"class.name\":\n                    current_class = content[node.start_byte : node.end_byte]\n                    class_node = node.parent\n\n                    # Extract class docstring\n                    docstring = self._extract_docstring(class_node, content)\n\n                    # Get class code\n                    code = content[class_node.start_byte : class_node.end_byte]\n\n                    # Find class dependencies\n                    dependencies = self._find_dependencies(class_node, content)\n\n                    element = CodeElement(\n                        name=current_class,\n                        type=\"class\",\n                        path=file_path,\n                        code=code,\n                        docstring=docstring,\n                        dependencies=dependencies,\n                    )\n                    self.elements[current_class] = element",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "_extract_docstring",
        "start_line": 121,
        "end_line": 132,
        "code": "def _extract_docstring(self, node, content: str) -> Optional[str]:\n        \"\"\"Extract docstring from a function or class node.\"\"\"\n        for child in node.children:\n            if child.type == \"block\":\n                for block_child in child.children:\n                    if block_child.type == \"expression_statement\":\n                        expr = block_child.children[0]\n                        if expr.type == \"string\":\n                            return content[expr.start_byte : expr.end_byte].strip(\n                                \"\\\"' \\n\"\n                            )\n        return None",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "_find_dependencies",
        "start_line": 134,
        "end_line": 150,
        "code": "def _find_dependencies(self, node, content: str) -> Set[str]:\n        \"\"\"Find all function/class dependencies in a node.\"\"\"\n        dependencies = set()\n\n        def visit_node(node):\n            if node.type == \"call\":\n                func_name = content[\n                    node.child_by_field_name(\"function\")\n                    .start_byte : node.child_by_field_name(\"function\")\n                    .end_byte\n                ]\n                dependencies.add(func_name)\n            for child in node.children:\n                visit_node(child)\n\n        visit_node(node)\n        return dependencies",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "visit_node",
        "start_line": 138,
        "end_line": 147,
        "code": "def visit_node(node):\n            if node.type == \"call\":\n                func_name = content[\n                    node.child_by_field_name(\"function\")\n                    .start_byte : node.child_by_field_name(\"function\")\n                    .end_byte\n                ]\n                dependencies.add(func_name)\n            for child in node.children:\n                visit_node(child)",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "build_dependency_graph",
        "start_line": 152,
        "end_line": 164,
        "code": "def build_dependency_graph(self):\n        \"\"\"Build a directed graph of dependencies.\"\"\"\n        self.dependency_graph.clear()\n\n        # Add nodes\n        for name, element in self.elements.items():\n            self.dependency_graph.add_node(name, **element.__dict__)\n\n        # Add edges\n        for name, element in self.elements.items():\n            for dep in element.dependencies:\n                if dep in self.elements:\n                    self.dependency_graph.add_edge(name, dep)",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "analyze_code_hierarchically",
        "start_line": 166,
        "end_line": 183,
        "code": "def analyze_code_hierarchically(self):\n        \"\"\"Analyze code elements from lowest to highest dependencies.\"\"\"\n        self.build_dependency_graph()\n\n        # Get elements in topological order (from least to most dependent)\n        try:\n            ordered_elements = list(nx.topological_sort(self.dependency_graph))\n        except nx.NetworkXUnfeasible:\n            print(\n                \"Warning: Circular dependencies detected. Using approximate ordering.\"\n            )\n            ordered_elements = list(self.elements.keys())\n\n        # Analyze each element\n        for element_name in ordered_elements:\n            element = self.elements[element_name]\n            if not element.analyzed:\n                self._analyze_element(element)",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "_analyze_element",
        "start_line": 185,
        "end_line": 209,
        "code": "def _analyze_element(self, element: CodeElement):\n        \"\"\"Generate analysis and description for a code element.\"\"\"\n        # Collect information about dependencies\n        dep_descriptions = []\n        for dep in element.dependencies:\n            if dep in self.elements:\n                dep_element = self.elements[dep]\n                if dep_element.description:\n                    dep_descriptions.append(f\"{dep}: {dep_element.description}\")\n\n        # This is where you'd normally call an LLM. For demo, we'll generate a simple description\n        description_parts = [\n            f\"Type: {element.type}\",\n            f\"Dependencies: {len(element.dependencies)}\",\n        ]\n\n        if element.docstring:\n            description_parts.append(f\"Documentation: {element.docstring}\")\n\n        if dep_descriptions:\n            description_parts.append(\"Uses:\")\n            description_parts.extend(f\"- {desc}\" for desc in dep_descriptions)\n\n        element.description = \"\\n\".join(description_parts)\n        element.analyzed = True",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "generate_report",
        "start_line": 250,
        "end_line": 318,
        "code": "def generate_report(self) -> dict:\n        \"\"\"Generate a hierarchical report of the codebase.\"\"\"\n        self.analyze_code_hierarchically()\n\n        report = {\n            \"elements\": {},\n            \"dependency_tree\": {},\n            \"statistics\": {\n                \"total_elements\": len(self.elements),\n                \"functions\": len(\n                    [e for e in self.elements.values() if e.type == \"function\"]\n                ),\n                \"classes\": len(\n                    [e for e in self.elements.values() if e.type == \"class\"]\n                ),\n                \"methods\": len(\n                    [e for e in self.elements.values() if e.type == \"method\"]\n                ),\n            },\n        }\n\n        def build_tree(element_name, visited=None):\n            \"\"\"Build dependency tree with cycle detection.\"\"\"\n            if visited is None:\n                visited = set()\n\n            if element_name in visited:\n                return {\"type\": \"circular_reference\", \"name\": element_name}\n\n            if element_name not in self.elements:\n                return {\"type\": \"unknown_reference\", \"name\": element_name}\n\n            visited.add(element_name)\n            element = self.elements[element_name]\n\n            tree = {\n                \"type\": element.type,\n                \"name\": element_name,\n                \"path\": element.path,\n                \"description\": element.description,\n                \"dependencies\": {},\n            }\n\n            for dep in element.dependencies:\n                if dep in self.elements:\n                    tree[\"dependencies\"][dep] = build_tree(dep, visited.copy())\n\n            return tree\n\n        # Add each top-level element to the report\n        for name, element in self.elements.items():\n            report[\"elements\"][name] = {\n                \"type\": element.type,\n                \"path\": element.path,\n                \"description\": element.description,\n                \"code\": element.code,\n                \"dependencies\": list(element.dependencies),\n            }\n\n            # Only build trees for top-level elements\n            if not element.parent_class:\n                report[\"dependency_tree\"][name] = build_tree(name)\n\n        # Add cycles information to the report\n        cycles = self._find_cycles()\n        if cycles:\n            report[\"cycles\"] = list(cycles)\n\n        return report",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "build_tree",
        "start_line": 271,
        "end_line": 297,
        "code": "def build_tree(element_name, visited=None):\n            \"\"\"Build dependency tree with cycle detection.\"\"\"\n            if visited is None:\n                visited = set()\n\n            if element_name in visited:\n                return {\"type\": \"circular_reference\", \"name\": element_name}\n\n            if element_name not in self.elements:\n                return {\"type\": \"unknown_reference\", \"name\": element_name}\n\n            visited.add(element_name)\n            element = self.elements[element_name]\n\n            tree = {\n                \"type\": element.type,\n                \"name\": element_name,\n                \"path\": element.path,\n                \"description\": element.description,\n                \"dependencies\": {},\n            }\n\n            for dep in element.dependencies:\n                if dep in self.elements:\n                    tree[\"dependencies\"][dep] = build_tree(dep, visited.copy())\n\n            return tree",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "_find_cycles",
        "start_line": 320,
        "end_line": 329,
        "code": "def _find_cycles(self) -> Set[tuple]:\n        \"\"\"Find and return all dependency cycles in the graph.\"\"\"\n        cycles = set()\n        try:\n            cycles = set(\n                tuple(cycle) for cycle in nx.simple_cycles(self.dependency_graph)\n            )\n        except Exception as e:\n            print(f\"Warning: Error detecting cycles: {str(e)}\")\n        return cycles",
        "file_path": "parser/parser.py"
      },
      {
        "type": "function",
        "name": "process_project_directory",
        "start_line": 7,
        "end_line": 104,
        "code": "def process_project_directory(\n    project_path: str,\n    exclude_dirs: Set[str] = None,\n    exclude_files: Set[str] = None,\n    output_file: Optional[str] = None,\n) -> dict:\n    \"\"\"\n    Process a Python project directory and analyze its code structure using CodeAnalyzer.\n\n    Args:\n        project_path (str): Path to the project root directory\n        exclude_dirs (Set[str]): Directories to exclude\n        exclude_files (Set[str]): Specific files to exclude\n        output_file (Optional[str]): Path to save the JSON report\n\n    Returns:\n        dict: Analysis report generated by CodeAnalyzer\n    \"\"\"\n    if exclude_dirs is None:\n        exclude_dirs = {\"venv\", \".venv\", \"__pycache__\", \".git\", \"node_modules\", \"DocAI\"}\n    if exclude_files is None:\n        exclude_files = {\"setup.py\"}\n\n    analyzer = CodeAnalyzer(root_path=project_path)\n    processed_files = []\n    errors = []\n\n    def should_process_dir(dir_name: str) -> bool:\n        return dir_name not in exclude_dirs\n\n    def should_process_file(file_name: str) -> bool:\n        return file_name.endswith(\".py\") and file_name not in exclude_files\n\n    # Process the project directory\n    for root, dirs, files in os.walk(project_path, topdown=True):\n        dirs[:] = [d for d in dirs if should_process_dir(d)]\n\n        for file in files:\n            if should_process_file(file):\n                file_path = os.path.join(root, file)\n                try:\n                    analyzer._parse_file(file_path)\n                    processed_files.append(file_path)\n                except Exception as e:\n                    error_info = {\"file\": file_path, \"error\": str(e)}\n                    errors.append(error_info)\n                    print(f\"Error processing {file_path}: {str(e)}\")\n\n    # Generate the analysis report\n    try:\n        report = analyzer.generate_report()\n\n        # Add processing metadata\n        report[\"processing_metadata\"] = {\n            \"processed_files\": processed_files,\n            \"total_files_processed\": len(processed_files),\n            \"errors\": errors,\n            \"excluded_directories\": list(exclude_dirs),\n            \"excluded_files\": list(exclude_files),\n        }\n\n        # Save report if output file is specified\n        if output_file:\n            try:\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(report, f, indent=2)\n                print(f\"Report saved to: {output_file}\")\n            except Exception as e:\n                print(f\"Error saving report: {str(e)}\")\n\n        return report\n\n    except RecursionError:\n        print(\n            \"Error: Maximum recursion depth exceeded. The project may have deeply nested or circular dependencies.\"\n        )\n        return {\n            \"error\": \"Maximum recursion depth exceeded\",\n            \"processing_metadata\": {\n                \"processed_files\": processed_files,\n                \"total_files_processed\": len(processed_files),\n                \"errors\": errors,\n                \"excluded_directories\": list(exclude_dirs),\n                \"excluded_files\": list(exclude_files),\n            },\n        }\n    except Exception as e:\n        print(f\"Error generating report: {str(e)}\")\n        return {\n            \"error\": str(e),\n            \"processing_metadata\": {\n                \"processed_files\": processed_files,\n                \"total_files_processed\": len(processed_files),\n                \"errors\": errors,\n                \"excluded_directories\": list(exclude_dirs),\n                \"excluded_files\": list(exclude_files),\n            },\n        }",
        "file_path": "parser/source.py"
      },
      {
        "type": "function",
        "name": "should_process_dir",
        "start_line": 34,
        "end_line": 35,
        "code": "def should_process_dir(dir_name: str) -> bool:\n        return dir_name not in exclude_dirs",
        "file_path": "parser/source.py"
      },
      {
        "type": "function",
        "name": "should_process_file",
        "start_line": 37,
        "end_line": 38,
        "code": "def should_process_file(file_name: str) -> bool:\n        return file_name.endswith(\".py\") and file_name not in exclude_files",
        "file_path": "parser/source.py"
      },
      {
        "type": "class",
        "name": "RagConfig",
        "start_line": 6,
        "end_line": 8,
        "code": "class RagConfig(AppConfig):\n    default_auto_field = \"django.db.models.BigAutoField\"\n    name = \"rag\"",
        "file_path": "rag/apps.py"
      },
      {
        "type": "class",
        "name": "RAGHandler",
        "start_line": 20,
        "end_line": 139,
        "code": "class RAGHandler:\n    def __init__(self):\n        self.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n        self.llm = \"codegemma\"\n        self.ollama_client = AsyncClient()\n        self.MONGODB_COLLECTION = self.setup_mongo()\n        self.vector_store = MongoDBAtlasVectorSearch(\n            collection=self.MONGODB_COLLECTION,\n            embedding=self.embeddings,\n            index_name=os.environ.get(\"ATLAS_VECTOR_SEARCH_INDEX_NAME\"),\n            relevance_score_fn=\"cosine\",\n        )\n        self.retriever = self.vector_store.as_retriever()\n        self.classifications = set()\n\n    def setup_mongo(self):\n        client = MongoClient(os.environ.get(\"MONGO_STRING\"))\n        DB_NAME = os.environ.get(\"DB_NAME\")\n        COLLECTION_NAME = os.environ.get(\"COLLECTION_NAME\")\n        print(DB_NAME, COLLECTION_NAME)\n        return client[DB_NAME][COLLECTION_NAME]\n\n    def preprocess_data(self, data):\n        sorted_functions = sorted(\n            data[\"dependency_tree\"].items(),\n            key=lambda item: len(item[1].get(\"dependencies\", {})),\n        )\n        sorted_info = [\n            {\n                \"name\": func_name,\n                \"code\": data[\"elements\"][func_name][\"code\"],\n                \"dependency_count\": len(\n                    data[\"dependency_tree\"][func_name].get(\"dependencies\", {})\n                ),\n                \"path\": data[\"elements\"][func_name][\"path\"],\n            }\n            for func_name, _ in sorted_functions\n        ]\n        return sorted_info\n\n    async def get_classification(\n        self, classification_classes, code, context=\"\", path=\"/\"\n    ):\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": PROMPT_CLASSIFICATION.format(\n                    classification_classes=classification_classes,\n                    Code=code,\n                    Context=context,\n                    path=path,\n                ),\n            },\n        ]\n        response = await self.ollama_client.chat(self.llm, messages=messages)\n        return response[\"message\"][\"content\"]\n\n    async def get_description(self, code, context=\"\"):\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": PROMPT_DESCRIPTION.format(Code=code, Context=context),\n            },\n        ]\n        response = await self.ollama_client.chat(self.llm, messages=messages)\n        return response[\"message\"][\"content\"]\n\n    async def get_embedding(self, text):\n        return self.embeddings.embed_query(text)\n\n    async def embed_documents(self, data):\n        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            chunk_size=500, chunk_overlap=100\n        )\n        sorted_info = self.preprocess_data(data)\n        classification_files = {}\n        os.makedirs(\"dummyOutput\", exist_ok=True)  # Move directory creation to start\n\n        for info in sorted_info:\n            print(f\"info : {info}\")\n            Context = \"\"\n            path = info[\"path\"]\n            if info[\"dependency_count\"] != 0:\n                docs = self.retriever.invoke(\n                    RAG_RETRIEVAL_PROMPT.format(Code=info[\"code\"])\n                )\n                Context = \"\".join([doc.page_content for doc in docs])\n            print(f\"Context : {Context}\")\n            description = await self.get_description(info[\"code\"], Context)\n            print(f\"description: {description}\")\n            classification_classes = \", \".join(self.classifications)\n            classification = await self.get_classification(\n                classification_classes, info[\"code\"], Context, path\n            )\n            self.classifications.add(classification)\n            print(f\"class_name: {classification}\")\n\n            doc_splits = text_splitter.create_documents([description])\n            print(f\"Docsplits: {doc_splits}\")\n\n            # Process each document split and save immediately\n            for doc in doc_splits:\n                doc_collection = {}\n                doc_collection[\"embedding\"] = await self.get_embedding(doc.page_content)\n                doc_collection[\"classification\"] = classification\n                doc_collection[\"_id\"] = str(uuid4())\n                doc_collection[\"path\"] = path\n                self.MONGODB_COLLECTION.insert_one(doc_collection)\n\n                # Prepare classification filename\n                safe_classification = classification.lower()\n                safe_classification = safe_classification.replace(\" \", \"_\")\n                safe_classification = safe_classification[:50]\n                filename = f\"dummyOutput/{safe_classification}.md\"\n\n                # Append content to classification file immediately\n                with open(filename, \"a\") as f:\n                    if not os.path.getsize(filename):  # If file is empty, add header\n                        f.write(f\"# {classification}\\n\\n\")\n                    f.write(f\"## {str(uuid4())[:8]}\\n\\n{doc.page_content}\\n\\n\")",
        "file_path": "rag/rag_handler.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "start_line": 21,
        "end_line": 33,
        "code": "def __init__(self):\n        self.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n        self.llm = \"codegemma\"\n        self.ollama_client = AsyncClient()\n        self.MONGODB_COLLECTION = self.setup_mongo()\n        self.vector_store = MongoDBAtlasVectorSearch(\n            collection=self.MONGODB_COLLECTION,\n            embedding=self.embeddings,\n            index_name=os.environ.get(\"ATLAS_VECTOR_SEARCH_INDEX_NAME\"),\n            relevance_score_fn=\"cosine\",\n        )\n        self.retriever = self.vector_store.as_retriever()\n        self.classifications = set()",
        "file_path": "rag/rag_handler.py"
      },
      {
        "type": "function",
        "name": "setup_mongo",
        "start_line": 35,
        "end_line": 40,
        "code": "def setup_mongo(self):\n        client = MongoClient(os.environ.get(\"MONGO_STRING\"))\n        DB_NAME = os.environ.get(\"DB_NAME\")\n        COLLECTION_NAME = os.environ.get(\"COLLECTION_NAME\")\n        print(DB_NAME, COLLECTION_NAME)\n        return client[DB_NAME][COLLECTION_NAME]",
        "file_path": "rag/rag_handler.py"
      },
      {
        "type": "function",
        "name": "preprocess_data",
        "start_line": 42,
        "end_line": 58,
        "code": "def preprocess_data(self, data):\n        sorted_functions = sorted(\n            data[\"dependency_tree\"].items(),\n            key=lambda item: len(item[1].get(\"dependencies\", {})),\n        )\n        sorted_info = [\n            {\n                \"name\": func_name,\n                \"code\": data[\"elements\"][func_name][\"code\"],\n                \"dependency_count\": len(\n                    data[\"dependency_tree\"][func_name].get(\"dependencies\", {})\n                ),\n                \"path\": data[\"elements\"][func_name][\"path\"],\n            }\n            for func_name, _ in sorted_functions\n        ]\n        return sorted_info",
        "file_path": "rag/rag_handler.py"
      },
      {
        "type": "function",
        "name": "get_classification",
        "start_line": 60,
        "end_line": 75,
        "code": "async def get_classification(\n        self, classification_classes, code, context=\"\", path=\"/\"\n    ):\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": PROMPT_CLASSIFICATION.format(\n                    classification_classes=classification_classes,\n                    Code=code,\n                    Context=context,\n                    path=path,\n                ),\n            },\n        ]\n        response = await self.ollama_client.chat(self.llm, messages=messages)\n        return response[\"message\"][\"content\"]",
        "file_path": "rag/rag_handler.py"
      },
      {
        "type": "function",
        "name": "get_description",
        "start_line": 77,
        "end_line": 85,
        "code": "async def get_description(self, code, context=\"\"):\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": PROMPT_DESCRIPTION.format(Code=code, Context=context),\n            },\n        ]\n        response = await self.ollama_client.chat(self.llm, messages=messages)\n        return response[\"message\"][\"content\"]",
        "file_path": "rag/rag_handler.py"
      },
      {
        "type": "function",
        "name": "get_embedding",
        "start_line": 87,
        "end_line": 88,
        "code": "async def get_embedding(self, text):\n        return self.embeddings.embed_query(text)",
        "file_path": "rag/rag_handler.py"
      },
      {
        "type": "function",
        "name": "embed_documents",
        "start_line": 90,
        "end_line": 139,
        "code": "async def embed_documents(self, data):\n        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n            chunk_size=500, chunk_overlap=100\n        )\n        sorted_info = self.preprocess_data(data)\n        classification_files = {}\n        os.makedirs(\"dummyOutput\", exist_ok=True)  # Move directory creation to start\n\n        for info in sorted_info:\n            print(f\"info : {info}\")\n            Context = \"\"\n            path = info[\"path\"]\n            if info[\"dependency_count\"] != 0:\n                docs = self.retriever.invoke(\n                    RAG_RETRIEVAL_PROMPT.format(Code=info[\"code\"])\n                )\n                Context = \"\".join([doc.page_content for doc in docs])\n            print(f\"Context : {Context}\")\n            description = await self.get_description(info[\"code\"], Context)\n            print(f\"description: {description}\")\n            classification_classes = \", \".join(self.classifications)\n            classification = await self.get_classification(\n                classification_classes, info[\"code\"], Context, path\n            )\n            self.classifications.add(classification)\n            print(f\"class_name: {classification}\")\n\n            doc_splits = text_splitter.create_documents([description])\n            print(f\"Docsplits: {doc_splits}\")\n\n            # Process each document split and save immediately\n            for doc in doc_splits:\n                doc_collection = {}\n                doc_collection[\"embedding\"] = await self.get_embedding(doc.page_content)\n                doc_collection[\"classification\"] = classification\n                doc_collection[\"_id\"] = str(uuid4())\n                doc_collection[\"path\"] = path\n                self.MONGODB_COLLECTION.insert_one(doc_collection)\n\n                # Prepare classification filename\n                safe_classification = classification.lower()\n                safe_classification = safe_classification.replace(\" \", \"_\")\n                safe_classification = safe_classification[:50]\n                filename = f\"dummyOutput/{safe_classification}.md\"\n\n                # Append content to classification file immediately\n                with open(filename, \"a\") as f:\n                    if not os.path.getsize(filename):  # If file is empty, add header\n                        f.write(f\"# {classification}\\n\\n\")\n                    f.write(f\"## {str(uuid4())[:8]}\\n\\n{doc.page_content}\\n\\n\")",
        "file_path": "rag/rag_handler.py"
      },
      {
        "type": "function",
        "name": "embed_documents_view",
        "start_line": 10,
        "end_line": 20,
        "code": "async def embed_documents_view(request):\n    if request.method == \"POST\":\n        try:\n            data = json.loads(request.body)\n            await rag_handler.embed_documents(data)\n            \n            return JsonResponse({\"status\": \"success\", \"message\": \"Data embedded successfully.\"})\n        except Exception as e:\n            return JsonResponse({\"status\": \"error\", \"message\": str(e)}, status=400)\n    else:\n        return JsonResponse({\"status\": \"error\", \"message\": \"Only POST requests are allowed.\"}, status=405)",
        "file_path": "rag/views.py"
      },
      {
        "type": "function",
        "name": "embed_json_file_view",
        "start_line": 23,
        "end_line": 43,
        "code": "async def embed_json_file_view(request):\n    if request.method == \"GET\":\n        try:\n            with open('dummyOutput/Op.json', 'r') as file:\n                data = json.load(file)\n                # Only take elements and dependency_tree from data\n                filtered_data = {\n                    'elements': data['elements'],\n                    'dependency_tree': data['dependency_tree']\n                }\n            await rag_handler.embed_documents(filtered_data)\n            \n            return JsonResponse({\"status\": \"success\", \"message\": \"JSON file embedded successfully.\"})\n        except FileNotFoundError:\n            return JsonResponse({\"status\": \"error\", \"message\": \"JSON file not found\"}, status=404)\n        except json.JSONDecodeError:\n            return JsonResponse({\"status\": \"error\", \"message\": \"Invalid JSON file\"}, status=400)\n        except Exception as e:\n            return JsonResponse({\"status\": \"error\", \"message\": str(e)}, status=400)\n    else:\n        return JsonResponse({\"status\": \"error\", \"message\": \"Only GET requests are allowed.\"}, status=405)",
        "file_path": "rag/views.py"
      },
      {
        "type": "function",
        "name": "setup_environment",
        "start_line": 14,
        "end_line": 37,
        "code": "def setup_environment():\n    \"\"\"Set up the necessary environment for local testing.\"\"\"\n    print(\"Setting up test environment...\")\n    \n    # Create necessary directories\n    os.makedirs(\".github/scripts\", exist_ok=True)\n    os.makedirs(\".docai\", exist_ok=True)\n    \n    # Copy the analyzer script to the correct location if it's not already there\n    analyzer_script = \".github/scripts/analyze_code_changes.py\"\n    if not os.path.exists(analyzer_script):\n        source_script = \"analyze_code_changes.py\"\n        if os.path.exists(source_script):\n            import shutil\n            shutil.copy(source_script, analyzer_script)\n            print(f\"Copied analyzer script to {analyzer_script}\")\n        else:\n            print(f\"ERROR: Analyzer script not found at {source_script}\")\n            return False\n    \n    # Make the script executable\n    os.chmod(analyzer_script, 0o755)\n    \n    return True",
        "file_path": "test_analyzer.py"
      },
      {
        "type": "function",
        "name": "run_analyzer",
        "start_line": 39,
        "end_line": 95,
        "code": "def run_analyzer(before_commit=None, after_commit=None):\n    \"\"\"Run the code analyzer with optional commit range.\"\"\"\n    # Set environment variables to simulate GitHub Actions\n    env = os.environ.copy()\n    \n    if before_commit and after_commit:\n        # Create a fake GitHub event file to simulate a push event\n        event_data = {\n            \"before\": before_commit,\n            \"after\": after_commit\n        }\n        \n        import json\n        import tempfile\n        \n        with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n            json.dump(event_data, f)\n            event_path = f.name\n        \n        env[\"GITHUB_EVENT_PATH\"] = event_path\n        env[\"GITHUB_EVENT_NAME\"] = \"push\"\n        \n        print(f\"Testing commit range: {before_commit} -> {after_commit}\")\n    \n    # Run the analyzer\n    script_path = os.path.join(os.getcwd(), \".github/scripts/analyze_code_changes.py\")\n    try:\n        result = subprocess.run(\n            [sys.executable, script_path],\n            env=env,\n            check=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        print(\"Analysis output:\")\n        print(result.stdout)\n        \n        if result.stderr:\n            print(\"Errors/Warnings:\")\n            print(result.stderr)\n        \n        # Clean up temp file if created\n        if \"GITHUB_EVENT_PATH\" in env:\n            os.unlink(env[\"GITHUB_EVENT_PATH\"])\n        \n        return True\n    except subprocess.CalledProcessError as e:\n        print(\"Analysis failed:\")\n        print(e.stdout)\n        print(e.stderr)\n        \n        # Clean up temp file if created\n        if \"GITHUB_EVENT_PATH\" in env:\n            os.unlink(env[\"GITHUB_EVENT_PATH\"])\n        \n        return False",
        "file_path": "test_analyzer.py"
      },
      {
        "type": "function",
        "name": "check_results",
        "start_line": 97,
        "end_line": 136,
        "code": "def check_results():\n    \"\"\"Check and display the analysis results.\"\"\"\n    docai_dir = Path(\".docai\")\n    if not docai_dir.exists():\n        print(\"ERROR: .docai directory not found.\")\n        return False\n    \n    # Check for code elements database\n    elements_db = docai_dir / \"code_elements.json\"\n    if elements_db.exists():\n        print(f\"Code elements database found: {elements_db}\")\n        \n        import json\n        try:\n            with open(elements_db, 'r') as f:\n                data = json.load(f)\n                print(f\"Total elements in database: {len(data.get('elements', {}))}\")\n        except json.JSONDecodeError:\n            print(\"ERROR: Invalid JSON in code elements database.\")\n    else:\n        print(\"WARNING: Code elements database not found.\")\n    \n    # Check for change reports\n    reports = list(docai_dir.glob(\"change_report_*.json\"))\n    if reports:\n        print(f\"Found {len(reports)} change reports:\")\n        for report in reports:\n            print(f\"  - {report.name}\")\n            \n            try:\n                with open(report, 'r') as f:\n                    data = json.load(f)\n                    stats = data.get(\"stats\", {})\n                    print(f\"    Added: {stats.get('added', 0)}, Modified: {stats.get('modified', 0)}, Deleted: {stats.get('deleted', 0)}\")\n            except json.JSONDecodeError:\n                print(f\"    ERROR: Invalid JSON in {report.name}\")\n    else:\n        print(\"No change reports found.\")\n    \n    return True",
        "file_path": "test_analyzer.py"
      },
      {
        "type": "function",
        "name": "main",
        "start_line": 138,
        "end_line": 161,
        "code": "def main():\n    parser = argparse.ArgumentParser(description=\"Test the code analyzer locally.\")\n    parser.add_argument(\"--before\", help=\"Starting commit hash\")\n    parser.add_argument(\"--after\", help=\"Ending commit hash (default: HEAD)\", default=\"HEAD\")\n    parser.add_argument(\"--setup-only\", action=\"store_true\", help=\"Only set up the environment, don't run the analyzer\")\n    parser.add_argument(\"--check-only\", action=\"store_true\", help=\"Only check the results, don't run the analyzer\")\n    \n    args = parser.parse_args()\n    \n    if not args.check_only:\n        if not setup_environment():\n            print(\"Environment setup failed.\")\n            return 1\n        \n        if not args.setup_only:\n            print(\"\\nRunning code analyzer...\")\n            if not run_analyzer(args.before, args.after):\n                print(\"Analyzer execution failed.\")\n                return 1\n    \n    print(\"\\nChecking results...\")\n    check_results()\n    \n    return 0",
        "file_path": "test_analyzer.py"
      },
      {
        "type": "class",
        "name": "UserConfig",
        "start_line": 6,
        "end_line": 8,
        "code": "class UserConfig(AppConfig):\n    default_auto_field = \"django.db.models.BigAutoField\"\n    name = \"user\"",
        "file_path": "user/apps.py"
      },
      {
        "type": "function",
        "name": "uslog",
        "start_line": 15,
        "end_line": 28,
        "code": "def uslog(request):\n    if request.method == 'POST':\n        username = request.POST['username']\n        password = request.POST['password']\n        user = authenticate(request, username=username, password=password)\n        if user is not None:\n            login(request, user)\n            messages.get_messages(request)\n            messages.success(request, 'You are now logged in')\n            RefreshToken.for_user(user) \n        else:\n            messages.get_messages(request)\n            messages.error(request, 'Invalid username or password')\n    return render(request, 'user/login.html')",
        "file_path": "user/views.py"
      },
      {
        "type": "function",
        "name": "register",
        "start_line": 32,
        "end_line": 47,
        "code": "def register(request):\n   if request.method == 'POST':\n       username=request.POST.get('username')\n       password=request.POST.get('password')\n       email=request.POST.get('email')\n       if User.objects.filter(username=username).exists():\n           messages.error(request, 'Username is taken')\n       elif User.objects.filter(email=email).exists():\n           messages.error(request, 'Email is taken')\n       else:\n            user = User.objects.create_user(username=username, password=password, email=email)\n            user.save()\n            RefreshToken.for_user(user)\n            messages.success(request, 'Account created')\n#             return render(request, 'user/reg.html')\n   return render(request, 'user/reg.html')",
        "file_path": "user/views.py"
      },
      {
        "type": "function",
        "name": "uslogout",
        "start_line": 49,
        "end_line": 52,
        "code": "def uslogout(request):\n    logout(request)\n    messages.success(request, 'You are now logged out')\n    #return render(request, 'user/logout.html')",
        "file_path": "user/views.py"
      },
      {
        "type": "function",
        "name": "in_session",
        "start_line": 68,
        "end_line": 77,
        "code": "def in_session(request):\n    token=request.POST.get('token') or request.GET.get('token','')\n    username=request.POST.get('username') or request.GET.get('username','')\n    if token and username:\n        request.session['git_token']=token\n        request.session['git_username']=username\n        return JsonResponse({'success': 'Session started'})\n        #return fetch_repos(request)\n    else:\n        return JsonResponse({'error': 'Token and username are required'})",
        "file_path": "user/views.py"
      },
      {
        "type": "function",
        "name": "fetch_repos",
        "start_line": 80,
        "end_line": 90,
        "code": "def fetch_repos(request):\n    token=request.session.get('git_token')\n    if not token:\n        return render(request, 'user/replist.html', {\"error\": \"Token is required\"})\n    url = \"https://api.github.com/user/repos\"\n    headers = {\"Authorization\": f\"token {token}\"}\n    response = requests.get(url, headers=headers)\n    if response.status_code == 200:\n        return render(request, 'user/replist.html', {'repos': response.json()})\n    else:\n        return render(request, 'user/replist.html', {\"error\": f\"Failed to fetch repos: {response.status_code}\"})",
        "file_path": "user/views.py"
      },
      {
        "type": "function",
        "name": "clone_repo",
        "start_line": 132,
        "end_line": 148,
        "code": "def clone_repo(request):\n    token=request.session.get('git_token')\n    repo_name=request.POST.get('repos')\n    username=request.session.get('git_username')\n    url=f\"https://{username}:{token}@github.com/{username}/{repo_name}.git\"\n    cur_dir=os.path.dirname(os.path.abspath(__file__))\n    clone_dir=os.path.join(cur_dir,'cloned',repo_name)\n    try:\n        if not os.path.exists(clone_dir):\n            Repo.clone_from(url,clone_dir)\n            print(\"Repo cloned\") \n        else:\n            print(\"Repo already exists\")  \n        return render(request,'user/repos.html')\n    except Exception as e:\n        print(str(e))  \n    return render(request,'user/repos.html')",
        "file_path": "user/views.py"
      }
    ],
    "modified": [],
    "deleted": []
  },
  "stats": {
    "added": 74,
    "modified": 0,
    "deleted": 0,
    "total_elements": 74
  },
  "metadata": {
    "repository": "DocAI",
    "commit_range": {
      "before": "aa49337d5b43ffb9b75cc282b7afdc1f0389b6c7",
      "after": "e6b0a377ac914c1bdbe6a8ea2e9e302ae40b1717"
    },
    "timestamp": "2025-03-17T09:11:29+05:30"
  }
}